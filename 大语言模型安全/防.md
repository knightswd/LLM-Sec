# 防

### 《SPML: A DSL for Defending Language Models Against Prompt Attacks》

 [阅读全文](http://arxiv.org/abs/2402.11755v1)

- **研究背景:** : 随着大型语言模型（LLMs）在自然语言应用中的广泛应用，基于指令的定义设计聊天机器人变得越来越重要。但是，部署后的聊天机器人定义是固定的，容易受到恶意用户的攻击，这强调了防止不道德应用和财务损失的必要性。

- **实现方法:** : 以往的研究探讨了用户提示对基于LLM的聊天机器人的影响，但针对特定应用聊天机器人的攻击防御方法尚未深入研究。本文提出了一种特定领域语言System Prompt Meta Language（SPML），用于精炼提示和监控输入到基于LLM的聊天机器人的用户输入。SPML能够主动检查攻击提示，确保用户输入与聊天机器人定义一致，以防止在LLM骨干上执行恶意操作，同时优化成本。与现有方法相比，SPML还通过编程语言能力简化了聊天机器人定义的制作，克服了自然语言设计的挑战。

- **研究贡献:** : 本文介绍了SPML，并提供了一个开创性的基准测试，包含1.8k个系统提示和20k个用户输入，为聊天机器人定义评估提供了首个语言和基准。实验表明，SPML在理解攻击者提示方面的能力超过了GPT-4、GPT-3.5和LLAMA模型。

- **研究方法:** : 本文提出了一种新的领域特定语言SPML，用于细化提示和监控LLM基础上聊天机器人的用户输入。SPML通过编程语言的功能来优化聊天机器人定义的制作过程，并通过主动检查用户输入来防御攻击提示。

- **具体表现:** : 通过在包含1.8k个系统提示和20k个用户输入的基准测试中进行实验，SPML展示了其在理解攻击者提示方面的能力，超越了GPT-4、GPT-3.5和LLAMA模型。这些性能结果支持了SPML旨在防御LLM聊天机器人攻击的目标。

### 《Reformatted Alignment》

 [阅读全文](http://arxiv.org/abs/2402.12219v1)

- **研究背景:** : 本文探讨了如何提高大型语言模型（LLMs）在微调数据上的质量，以更好地与人类价值观对齐。目前提升数据质量的方法要么工作量巨大，要么容易因LLMs的幻觉错误而导致事实错误。

- **实现方法:** : 过去的方法存在人工注释繁琐和扩展困难的问题，以及LLMs幻觉导致的事实错误。本文提出了一种名为ReAlign的新方法，通过将指令数据的响应重新格式化，以更好地符合预设标准和汇总证据，从而提高数据质量。ReAlign方法减少了人工注释的需求，降低了幻觉错误，并且易于扩展，与现有对齐技术正交。

- **研究贡献:** : 本文的ReAlign方法显著提高了LLMs的一般对齐能力、数学推理、事实性和可读性。此外，该方法不需要引入额外数据或高级训练技术，仅通过重新格式化响应即可实现。

- **研究方法:** : 本文采用的ReAlign方法通过重新格式化指令数据的响应，使其更好地符合人类价值观和证据，从而提高LLMs的对齐质量。该方法的有效性通过实验得到了验证。

- **具体表现:** : 使用ReAlign方法后，LLaMA-2-13B在GSM8K数学推理任务上的准确率从46.77%提高到了56.63%。仅使用5%的ReAlign数据，就能使得在Alpaca数据集上测量的一般对齐能力提升67%。这些性能表现支持了本文的研究目标。

### 《AI-powered patching: the future of automated vulnerability fixes》

 [阅读全文](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/4fd3441fe40bb74e3f94f5203a17399af07b115c.pdf)

AI进行自动化漏洞修复

### 《ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection》

 [阅读全文](http://arxiv.org/abs/2402.18093v1)

- **研究背景:** : 由于网络钓鱼网站和邮件的激增，现有的网络安全措施面临重大挑战。尽管垃圾邮件过滤器和电子邮件安全协议有所进步，但监管不足和误报问题仍然存在。

- **实现方法:** : 过去的方法存在监管不足和误报问题，用户往往难以理解邮件被标记为垃圾邮件的原因。本文提出的ChatSpamDetector系统利用大型语言模型（LLMs）来检测钓鱼邮件，与现有方法相比，提供了详细的推理过程，帮助用户做出明智的决策。

- **研究贡献:** : 本文引入了ChatSpamDetector系统，该系统使用GPT-4等大型语言模型检测钓鱼邮件，并提供了高度准确的判断和详细的推理。

- **研究方法:** : 通过将电子邮件数据转换为适合LLMs分析的提示，进行了综合性钓鱼邮件数据集的评估，并将系统与多个LLMs和基线系统进行了比较。

- **具体表现:** : ChatSpamDetector使用GPT-4实现了99.70%的高准确率，通过LLMs的高级上下文解释能力，有效识别各种钓鱼策略和冒充行为，性能支持其目标。

### 《AI-assisted Tagging of Deepfake Audio Calls using Challenge-Response》

 [阅读全文](http://arxiv.org/abs/2402.18085v1)

- **研究背景:** : 由于AI语音克隆技术在社会工程攻击中的广泛应用，尤其是实时深度伪造（Real-time Deepfakes, RTDFs）技术的出现，使得电话通话中的语音克隆变得更加逼真，交互性强，诈骗手段更加难以识别。

- **实现方法:** : 过去的方法主要集中在深度伪造检测上，但对RTDF威胁效果不佳。本文提出了一种基于挑战-响应的方法来检测深度伪造音频通话，并创造了一套全面的音频挑战分类。与现有方法相比，该方法通过结合人类直觉和机器精度，提高了检测能力。

- **研究贡献:** : 本文提出了20个潜在的挑战，并针对领先的语音克隆系统进行了评估。创建了一个包含100名智能手机和桌面用户贡献的新颖开源挑战数据集，包含18,600个原始样本和1.6百万个深度伪造样本。此外，开发了一种创新的人工智能协作系统，提高了检测的准确性。

- **研究方法:** : 通过机器和人类对挑战数据集的严格评估，采用了11个挑战来显著提高检测能力。研究结合了人类的判断力和算法的准确性，通过人工智能辅助预筛选来提升通话验证过程。

- **具体表现:** : 该方法在深度伪造检测任务上达到了86%的检测率和80%的AUC分数。通过结合人工智能，最终的联合准确率提高到了82.9%。这些性能结果支持了他们的研究目标，即通过人工智能辅助提高电话通话验证的准确性。

### 《Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models》

 [阅读全文](http://arxiv.org/abs/2402.18059v1)

- **研究背景:** : 由于大型语言模型（Large Language Models, LLMs）生成的高质量回应可能包含错误信息，区分AI生成文本与人类撰写文本变得至关重要，水印技术在此扮演关键角色，通过在LLM推理阶段嵌入对人类不易察觉的标记来实现。

- **实现方法:** : 过去的水印算法在提高水印可检测性和保持文本语义完整性之间难以兼顾，增强一方面往往会削弱另一方面。本文提出的方法采用多目标优化（Multi-Objective Optimization, MOO）技术，通过轻量级网络生成特定于令牌的水印logits和分割比率，同时优化检测和语义目标函数，解决了这一问题。

- **研究贡献:** : 本文提出了一种新颖的水印技术，能够在不牺牲语义连贯性的情况下提高LLMs生成文本的水印可检测性。实验结果表明，该方法在保持语义一致性的同时，提高了水印的可检测性，超越了现有技术。

- **研究方法:** : 本文采用的是一种基于多目标优化的水印技术，利用轻量级网络来生成与特定令牌相关的水印logits和分割比率，以此来优化文本的水印可检测性和语义连贯性。

- **具体表现:** : 通过实验验证，本文方法在保持语义连贯性的同时，显著提高了LLMs生成文本的水印可检测性，实验结果支持了研究目标。代码已发布在GitHub（https://github.com/mignonjia/TS_watermark）上。

### 《Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning》

 [阅读全文](https://arxiv.org/pdf/2402.06255.pdf)

```commandline
Yichuan Mo 等人提出了一种新的方法，名为Prompt Adversarial Tuning（PAT），以保护大型语言模型（LLMs）不产生有害信息。这种方法通过训练一个防御控制机制来实现，该机制被嵌入到用户提示的前缀中，以实施我们的防御策略。作者设计了一个类似于对抗训练的训练过程，以实现优化目标，通过交替更新攻击和防御控制来进行。研究结果将提示注入攻击的成功率几乎降低到0，同时将简单良性问题的良性答案率保持在80%以上。

```

### 《Defending Jailbreak Prompts via In-Context Adversarial Game》

 [阅读全文](http://arxiv.org/abs/2402.13148v1)

- **研究背景:** : 大型语言模型（LLMs）在多种应用中展现出卓越能力，但其安全性问题，尤其是对越狱攻击的脆弱性，仍然存在关切。

- **实现方法:** : 过去的方法依赖于静态数据集进行防御，存在无法适应新生成的越狱提示的问题。本文提出的In-Context Adversarial Game（ICAG）方法，灵感来源于深度学习中的对抗性训练和LLMs代理学习过程，无需微调即可防御越狱攻击。ICAG通过代理学习进行对抗性游戏，动态扩展知识以抵御越狱攻击，并通过迭代过程增强防御和攻击代理，解决了传统方法的局限性。

- **研究贡献:** : 本文证实了ICAG的有效性，通过ICAG保护的LLMs在各种攻击场景下越狱成功率显著降低。此外，ICAG还展现出对其他LLMs的显著迁移能力，表明其作为一种通用防御机制的潜力。

- **研究方法:** : 本文采用了一种迭代的对抗性游戏方法，通过代理学习不断提升防御和攻击策略，以此来增强LLMs对越狱攻击的防御能力。

- **具体表现:** : 通过实证研究，ICAG保护下的LLMs在不同的攻击场景中越狱成功率大幅下降，证明了ICAG的防御效果。此外，ICAG对其他LLMs的迁移能力表明了其作为防御机制的有效性和通用性。

### 《Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code》

 [阅读全文](https://arxiv.org/abs/2311.07989)

```commandline
这篇论文对近年来使用语言模型进行代码处理的研究进行了系统性的回顾，涵盖了50多个模型、30多个评估任务和500多篇相关论文。作者将这些代码处理模型分为两大类：一类是通用语言模型，以GPT家族为代表；另一类是专门针对代码进行预训练的专用模型，通常采用定制化的目标函数。文章讨论了这些模型之间的关系和差异，并强调了代码建模从统计模型和RNNs到预训练Transformers和LLMs的历史演变，这正是自然语言处理领域所经历的发展历程。此外，文章还讨论了代码特有的特征，如抽象语法树（AST）、控制流图（CFG）和单元测试，以及它们在训练代码语言模型中的应用。最后，作者指出了这个领域面临的主要挑战和潜在的未来研究方向。这篇论文的更新版在GitHub仓库中持续维护，地址为https://github.com/codefuse-ai/Awesome-Code-LLM。

```

### 《Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline》

 [阅读全文](http://arxiv.org/abs/2403.03265v1)

- **研究背景:** : 传统的网络威胁情报（Cyber Threat Intelligence, CTI）方法难以跟上日益复杂的网络威胁演变，人工智能（Artificial Intelligence, AI）提供了一种可能的解决方案，能够自动化并增强从数据摄取到韧性验证的各种任务。

- **实现方法:** : 过去的方法依赖于人工处理和分析，存在速度慢、效率低和无法处理大规模数据的问题。本文提出的AI增强的CTI处理流程与现有方法相比，通过整合AI技术，能够自动化处理数据、生成缓解建议，并提供实时、上下文相关和预测性的洞察。该方法通过结合AI和人类专家的协作，解决了传统方法的问题，并且有充分的动机。

- **研究贡献:** : 本文提供了一个AI增强的CTI处理流程的蓝图，并详细描述了其组成部分和功能。同时探讨了自动化生成缓解建议的可能性，并讨论了AI集成到CTI中的挑战，包括伦理困境、潜在偏见以及AI驱动决策的透明度需求。

- **研究方法:** : 本文探索了AI与CTI集成的潜力，提出了一个包含多个组件和功能的AI增强CTI处理流程，并讨论了数据隐私、同意机制、技术潜在滥用、偏见问题和透明度的重要性。

- **具体表现:** : 文中没有提供具体的性能评估数据，但强调了AI增强的CTI处理流程能够提供实时、上下文相关和预测性的洞察，这支持了其目标，即提高网络防御能力和优化人工智能协作。

### 《LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors》

 [阅读全文](https://www.ndss-symposium.org/wp-content/uploads/2024-238-paper.pdf)

```commandline
提出了 LMSanitor，这是一种用于检测和删除 Transformer 模型上与任务无关的后门的新方法。LMSaniator 的目标不是直接反转触发器，而是反转与任务无关的后门的“预定义攻击向量”（当输入嵌入触发器时预训练模型的输出），从而实现更好的收敛性能和后门检测精度。

```

### 《Asset-driven Threat Modeling for AI-based Systems》

 [阅读全文](http://arxiv.org/abs/2403.06512v1)

- **研究背景:** : 本文探讨了如何为依赖人工智能（Artificial Intelligence, AI）的系统进行威胁建模，这是一种通过识别潜在的安全威胁来保障系统安全开发的方法。然而，现有的威胁建模方法并未充分考虑AI相关的威胁。

- **实现方法:** : 过去的方法没有针对AI系统的特殊威胁进行设计，存在的问题包括缺乏引导和自动化威胁识别的能力，以及缺乏实践证明其有效性。本文提出的方法与现有方法的不同之处在于，它能够在系统架构定义阶段引导并自动识别与AI相关的威胁。该方法通过实践证明了其有效性，并且得到了专家的认可。

- **研究贡献:** : 本文的贡献在于提出了一种能够在设计阶段自动识别AI系统威胁的方法，并通过在医疗领域的AI系统设计中的应用，验证了该方法的实用性和有效性。

- **研究方法:** : 本文采用了专家评估的方式，邀请多位专家对一个在医疗领域设计的AI系统进行威胁建模，以评估所提出方法的可用性和有效性。

- **具体表现:** : 通过专家评估，结果表明本文提出的威胁建模方法在威胁识别方面是有效的，且该方法的可用性得到了良好的反馈。这些表现支持了本文的研究目标，即在系统设计阶段自动识别AI相关威胁。

### 《Threats, Attacks, and Defenses in Machine Unlearning: A Survey》

 [阅读全文](http://arxiv.org/abs/2403.13682v1)

- **研究背景:** : 本文关注于Machine Unlearning (MU) 的安全性，特别是在数据敏感性、版权限制、过时或低质量问题以及遵守隐私法规（如Right To Be Forgotten, RTBF）方面的应用。MU的目标是从训练有素的机器学习（ML）模型中移除特定数据的影响，以减少不良后果的风险，提高AI系统的伦理使用和可靠性。

- **实现方法:** : 过去的方法主要集中在设计高效的unlearning方法上，以及如何将MU服务整合到现有的Machine Learning as a Service (MLaaS)中。但是，这些方法存在安全性和隐私方面的问题，如信息泄露和恶意的unlearning请求。本文提出的方法与现有方法的不同之处在于，它不仅关注unlearning方法本身，还关注这些方法在MU系统中的多样化角色，如如何作为恢复模型免受后门攻击的机制，以及如何将后门攻击作为评估unlearning效果的指标。

- **研究贡献:** : 本文首次提供了一个全面的关于机器遗忘中的威胁、攻击和防御的综述，分类了它们的分类法、方法和解决方案，并为未来的研究方向和实际应用提供了宝贵的见解。

- **研究方法:** : 本文采用的研究方法是文献综述，通过分析和总结大量关于MU的研究，来揭示不同unlearning方法和攻击之间的复杂关系以及它们在维持系统功能和安全中的相互作用。

- **具体表现:** : 文中并未提及具体的实验任务或性能评估结果，而是侧重于对现有研究的总结和分析。因此，无法提供具体的性能支持其目标的数据。

### 《Have You Poisoned My Data? Defending Neural Networks against Data Poisoning》

 [阅读全文](http://arxiv.org/abs/2403.13523v1)

- **研究背景:** : 本文研究了在大量训练数据推动神经网络快速发展的背景下，数据中毒攻击（poisoning attacks）对模型的潜在威胁。这种攻击通过对训练数据进行恶意操作，以达到破坏学习模型的目的。

- **实现方法:** : 过去的方法存在一些问题，如难以区分被污染的数据点。本文提出了一种新的方法，通过定义新的特征向量表示数据点，在特征向量空间中有效区分有害数据点和干净数据点。与现有方法相比，本文方法在防御率和最终模型性能上都有所提高，更好地解决了数据中毒问题。

- **研究贡献:** : 本文提出了一种新的防御清洁标签中毒攻击的方法，并在迁移学习（transfer learning）设置中检测和过滤被污染的数据点。通过实验分析，证明了该方法在多种架构、数据集和中毒预算下的有效性。

- **研究方法:** : 本文定义了一种新的数据点的特征向量表示，并通过实验分析展示了其如何有效捕获数据分布的内在属性。同时，本文还对所提出的方法进行了全面评估，并与现有的最先进防御方法进行了比较。

- **具体表现:** : 在多种架构、数据集和中毒预算的实验设置中，本文方法在防御率和最终训练模型性能方面均优于现有方法。这些性能结果支持了本文的研究目标，证明了所提方法的有效性。

### 《Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark》

 [阅读全文](http://arxiv.org/abs/2403.13502v2)

- **研究背景:** : 本文探讨了将机器学习集成到自动控制系统（Automated Control Systems, ACS）中，用于工业过程管理决策的优势与挑战。特别是，研究了神经网络在对抗性攻击面前的脆弱性，这是阻碍这些技术在工业界广泛应用的一个限制因素。

- **实现方法:** : 过去的方法主要是单一的防御策略，但存在效果不一和容易被绕过的问题。本文提出了一种新的防御方法，通过结合多种防御手段来增强模型的鲁棒性。与现有方法相比，这种组合防御方法更能有效地抵御对抗性攻击，并且有很好的动机。

- **研究贡献:** : 本文评估了三种不同架构的神经网络在Tennessee Eastman Process数据集上对六种类型的对抗性攻击的脆弱性，并探索了五种不同的防御方法。研究提出了一种结合多重防御手段的新型保护方法，并证明了其有效性。这为确保机器学习在自动控制系统中的安全，保障工业过程中的稳健故障诊断提供了重要见解。

- **研究方法:** : 本文采用实验评估的方法，通过在Tennessee Eastman Process数据集上测试不同的神经网络模型，分析它们对不同对抗性攻击的反应，并评估多种防御策略的效果。

- **具体表现:** : 通过实验，本文展示了模型对对抗性样本的高度脆弱性以及防御策略的不同效果。提出的组合防御方法在提高模型鲁棒性方面表现出色，能够支持研究的目标，确保自动控制系统中的机器学习模型能够抵御对抗性攻击，保障工业过程的稳健故障诊断。

### [风险综述]《Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices》

 [阅读全文](http://arxiv.org/abs/2403.12503v1)

- **研究背景:** : 随着大型语言模型（LLMs）在自然语言处理（NLP）领域的广泛应用，它们在语言理解和生成任务上展现出显著的能力。然而，LLMs的使用也带来了安全性和隐私方面的风险，需要对这些潜在的威胁和漏洞进行深入研究，以确保负责任地部署和使用。

- **实现方法:** : 本文分析了现有的安全和隐私问题，以及LLMs面临的对抗性攻击的脆弱性。同时指出了现有方法的局限性，并提出了新的缓解策略。这些策略旨在解决LLMs可能被滥用造成的危害，并强调了现有策略的不足。提出的方法在动机上是合理的，旨在通过全面的安全和风险管理来增强LLMs的安全性。

- **研究贡献:** : 本文从五个主题角度出发，全面调查了LLMs相关的安全和隐私问题，包括对抗性攻击的脆弱性、LLMs滥用可能造成的危害、缓解策略及其局限性，并为未来研究提供了有前景的方向，以增强LLMs的安全性和风险管理。

- **研究方法:** : 本文采用了全面的研究方法，包括对现有文献的系统性回顾、安全和隐私问题的分类分析、对抗性攻击的案例研究，以及缓解策略的评估和推荐。

- **具体表现:** : 文中并未提供具体的性能评估或实验结果，而是侧重于理论分析和策略建议。因此，无法直接评估方法的性能是否支持其目标，但提供的分析和建议对于指导未来的研究和实践具有重要价值。

### [风险评估]《Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal》

 [阅读全文](http://arxiv.org/abs/2403.13309v1)

- **研究背景:** : 随着大型语言模型（LLMs）在各个领域的快速集成，虽然在文本生成和问题解决任务上展现出了卓越能力，但也带来了显著的风险和漏洞。攻击者不断利用这些弱点，使得LLMs的整体可信度受到质疑。

- **实现方法:** : 过去的研究如OWASP和MITRE提供了威胁和漏洞的概述，但缺乏一种直接且简洁的风险分析方法。本文提出了一种风险评估过程，使用OWASP风险评级方法对传统系统进行评估，并通过情景分析识别潜在的威胁代理，将系统组件映射到漏洞因素上，评估网络攻击的可能性，并进行深入的影响分析，以形成全面的威胁矩阵。该方法解决了现有方法的问题，并为安全从业者提供了行动指南。

- **研究贡献:** : 本文提出了一个全面的LLM相关风险评估威胁矩阵，针对三个关键利益相关者群体：进行模型微调的开发者、使用第三方API的应用开发者和最终用户。该威胁矩阵为利益相关者提供了全面的风险评估，帮助他们做出有根据的决策，并制定有效的缓解策略。

- **研究方法:** : 本文采用了OWASP风险评级方法进行风险评估，并通过情景分析来识别潜在的威胁代理，映射依赖的系统组件与漏洞因素，进行可能性评估和影响分析，以建立威胁矩阵。

- **具体表现:** : 本文方法通过建立威胁矩阵，为安全从业者提供了一个可行动的、全面的工具，用于资源管理和提升整体系统安全。虽然没有提供具体的性能指标，但所提出的方法和工具被认为能够支持他们的目标，即为LLMs的安全性提供更好的评估和缓解策略。

### [漏洞修复]《A Study of Vulnerability Repair in JavaScript Programs with Large Language Models》

 [阅读全文](http://arxiv.org/abs/2403.13193v1)

- **研究背景:** : JavaScript作为当前最广泛使用的编程语言，尤其在Web开发中，但编写安全的JavaScript代码具有挑战性，常常导致安全漏洞。大型语言模型（LLMs），如ChatGPT和Bard，在代码自动生成和修复方面展现出潜力。

- **实现方法:** : 过去的方法侧重于手动或半自动的漏洞修复，存在效率低下和准确性不足的问题。本文提出的方法利用LLMs进行自动化的JavaScript程序漏洞修复，并探讨了在提示中加入上下文对修复正确性的影响。与传统方法相比，LLMs提供了一种更快速、可能更准确的修复策略。

- **研究贡献:** : 本文探索了LLMs在发现和修复JavaScript程序安全漏洞方面的准确性，并研究了提示中上下文的影响。实验结果表明，适当的上下文可以显著提高LLMs生成正确补丁的概率。

- **研究方法:** : 本文通过对真实世界软件漏洞的实验，评估了LLMs在JavaScript代码自动程序修复中的表现，并分析了不同上下文提示对修复结果的影响。

- **具体表现:** : 实验表明LLMs在自动程序修复JavaScript代码方面具有潜力，但要实现正确的修复，需要在提示中提供适当的上下文。尽管没有提供具体的数值表现，但研究结果支持LLMs在自动化代码修复中的应用目标。

### [机器遗忘]《Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects》

 [阅读全文](http://arxiv.org/abs/2403.12830v1)

- **研究背景:** : 本文关注数据隐私和安全问题，特别是机器学习模型中的数据遗留问题，强调了机器遗忘（machine unlearning）的必要性，以便完全移除数据的影响，以符合监管要求。

- **实现方法:** : 过去的方法在验证机器遗忘的有效性方面进展缓慢，缺乏强有力的方法。本文提出了一种新的方法，将审计挑战转化为非成员推断问题，并开发了有效的审计指标，这些指标仅依赖原始模型和遗忘后的模型，简化了对单个数据点遗忘的评估。与现有方法相比，本文方法避免了训练额外的影子模型，更直接有效地解决了问题。

- **研究贡献:** : 本文为黑盒遗忘审计任务引入了明确定义和有效的指标，对当前的近似机器遗忘算法进行了深入分析，并指出了这些方法在效用、韧性和公平性方面的不足。

- **研究方法:** : 本文通过将审计挑战转化为非成员推断问题，开发了一套高效的审计指标，并利用这些指标对现有的近似机器遗忘算法进行了评估。

- **具体表现:** : 本文通过实验验证了所提出的审计指标，分析了现有近似机器遗忘算法的不足，并未提供具体的数值表现。但所提出的方法有助于将理论上的数据擦除权转化为可审计的现实，支持了他们的目标。

### [python fuzz]《Python Fuzzing for Trustworthy Machine Learning Frameworks》

 [阅读全文](http://arxiv.org/abs/2403.12723v1)

- **研究背景:** : 为了构建可信赖的基于AI的系统，确保机器学习框架的安全性和可靠性至关重要。Python Fuzzing是一种在安全软件开发生命周期（SSDLC）中常用的技术，用于开发安全且健壮的软件。

- **实现方法:** : 过去的方法主要是针对C/C++等语言的Fuzzing技术，但对于Python这样的语言支持不足，且未能有效整合到持续集成流程中。本文提出了一种针对Python项目的动态分析流水线，使用Sydr-Fuzz工具集，包括Fuzzing、语料库最小化、崩溃分类和覆盖率收集。与现有方法相比，本文方法特别关注崩溃分类和严重性评估，以确保及时处理最关键的漏洞，并且将该流水线集成到GitLab CI中。

- **研究贡献:** : 本文分析了流行的机器学习框架如PyTorch和TensorFlow的潜在攻击面，并为它们及相关项目（如h5py）开发了Fuzz目标。通过应用动态分析流水线，作者成功发现了3个新的漏洞并提出了修复方案。

- **研究方法:** : 本文提出了一种动态分析流水线，包括使用Sydr-Fuzz进行Fuzzing，然后进行语料库最小化、崩溃分类、严重性评估和覆盖率收集。此外，将该流水线集成到GitLab CI中，以实现自动化的漏洞检测和修复。

- **具体表现:** : 应用本文提出的方法，对PyTorch、TensorFlow等机器学习框架进行了测试，发现了3个新的漏洞。这些成果证明了该方法能够有效地识别和修复机器学习框架中的安全问题，支持了其目标——提高机器学习框架的安全性和可靠性。

[代码安全]《Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering》

 [阅读全文](http://arxiv.org/abs/2403.12671v1)

- **研究背景:** : 随着AI辅助编程工具如GitHub Copilot的兴起，开发者和公司对于这些工具生成代码的安全性表示担忧，这限制了它们的广泛应用。

- **实现方法:** : 过去的方法主要依赖于代码安全审计，但这需要专家知识，且成本高昂。本文提出了一种基于提示工程(prompt-engineering)的方法，通过修改提示来提高AI代码生成器的安全性，与现有方法相比，这种方法用户友好、成本低廉、不需要专家知识。提出的方法包括：场景特定提示、迭代提示和通用条款提示，并探讨了它们的组合使用。

- **研究贡献:** : 本文提出了三种提高AI代码生成器安全性的提示工程方法，并证明了这些方法在不需要访问AI模型内部的情况下，对任何AI代码合成器都是通用的。

- **研究方法:** : 本文采用系统化的方法评估GitHub Copilot在OpenVPN项目中的表现，通过实际场景测试来验证所提方法的有效性。

- **具体表现:** : 应用所提方法后，不安全代码样本数量减少了高达16%，安全代码样本数量增加了高达8%。这些性能指标支持了研究目标，即通过简单有效的提示工程提高代码生成的安全性。

### [水印]《Towards Better Statistical Understanding of Watermarking LLMs》

 [阅读全文](http://arxiv.org/abs/2403.13027v1)

- **研究背景:** : 本文研究了大型语言模型(LLMs)的水印技术问题，探讨了模型失真与检测能力之间的权衡，并将其形式化为一个基于Kirchenbauer等人(2023a)的绿红算法的约束优化问题。

- **实现方法:** : 过去的方法存在模型失真和检测能力不平衡的问题。本文提出的方法与现有方法的不同之处在于，它通过优化问题的解析性质来指导水印算法的设计，并通过在线对偶梯度上升算法实现模型失真与检测能力之间的渐进帕累托最优性。该方法明确提高了绿名单概率，从而提高了检测能力。

- **研究贡献:** : 本文的贡献在于开发了一种基于优化公式的在线对偶梯度上升水印算法，并证明了其在模型失真与检测能力之间的渐进帕累托最优性。此外，系统讨论了水印问题中模型失真度量的选择，并提出了使用KL散度的理由，同时指出了现有的“无失真”和困惑度标准的问题。

- **研究方法:** : 本文采用了基于约束优化问题的在线对偶梯度上升算法，通过解析性质指导算法设计，并对算法的帕累托最优性进行了理论证明。

- **具体表现:** : 本文的方法在广泛的数据集上进行了实证评估，并与基准算法进行了对比。结果显示，该算法在提高绿名单概率和检测能力方面取得了显著效果，支持了其目标的实现。

### [挑战&机遇]《Large language models in 6G security: challenges and opportunities》

 [阅读全文](http://arxiv.org/abs/2403.12239v1)

- **研究背景:** : 本文探讨了Generative AI (GenAI)和Large Language Models (LLMs)在教育和医疗等行业的快速整合，以及这些技术在6G安全领域中所面临的挑战和机遇。

- **实现方法:** : 过去的方法主要集中在传统的网络安全防御手段，但随着LLMs的出现，安全攻击面扩大，现有方法未能充分解决新的安全威胁。本文提出的方法是从潜在敌手的视角分析LLMs的安全弱点，并开发全面的威胁分类，同时探讨LLMs与区块链技术的结合，以发展下一代全自动安全解决方案。

- **研究贡献:** : 本文的贡献在于提出了一个针对LLMs安全性的威胁分类体系，并探索了LLMs在网络安全防御中的应用潜力，以及LLMs与区块链技术结合的新策略，旨在建立一个统一的网络安全策略，增强整体数字安全基础设施。

- **研究方法:** : 本文采用了深入分析已知安全弱点的方法，构建威胁分类，并研究LLMs与区块链技术的结合可能性，以及这种结合如何促进全自动安全解决方案的发展。

- **具体表现:** : 文中并未提供具体的性能评估数据，但提出的方法旨在通过建立统一的网络安全策略来支持其目标，即增强数字安全基础设施。

### [投毒攻击防护]《Diffusion Denoising as a Certified Defense against Clean-label Poisoning》

 [阅读全文](http://arxiv.org/abs/2403.11981v1)

- **研究背景:** : 本文针对清洁标签投毒攻击（clean-label poisoning attacks）提出了一种认证防御机制。这种攻击通过向训练数据中注入少量（例如1%）含有$p$-范数界限的对抗性扰动样本，以诱导测试时输入的目标错误分类。

- **实现方法:** : 过去的方法存在一定的局限性，无法有效防御清洁标签投毒攻击或在提高防御效果的同时牺牲了模型的实用性。本文提出的方法与现有方法不同，它受到了$denoised$ $smoothing$在对抗鲁棒性方面的启发，使用现成的扩散模型（diffusion model）来净化被篡改的训练数据。该方法旨在解决现有方法的问题，并且有很好的动机。

- **研究贡献:** : 本文的贡献在于提出了一种对抗清洁标签投毒攻击的认证防御机制，并通过实验降低了七种清洁标签投毒攻击的成功率至0-16%，同时仅对测试时的准确率造成了微不足道的影响。与现有的防御措施相比，本文的方法在降低攻击成功率和保持模型效用方面表现最佳。

- **研究方法:** : 本文采用的研究方法是利用扩散模型对训练数据进行去噪处理，以此来清除训练集中的投毒样本。通过这种方式，可以在不显著影响模型性能的前提下，有效地减少攻击样本对模型的影响。

- **具体表现:** : 在对抗清洁标签投毒攻击的任务上，本文方法将攻击成功率降至0-16%，并且测试时的准确率仅有轻微下降。这一性能表现支持了他们的目标，即开发出一种既认证又实用的防御机制，为未来更强的清洁标签攻击提供了一个强有力的基准。

### [音频检测]《Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms》

 [阅读全文](http://arxiv.org/abs/2403.11778v1)

- **研究背景:** : 通讯平台中的Deepfake音频造假技术日益成为威胁，需要实时检测以确保音频流的完整性。

- **实现方法:** : 传统方法非实时且存在局限性。本文提出了一种基于Resnet和LCNN架构的实时Deepfake音频检测模型，并开发了一个可跨平台兼容的可执行软件。与传统方法相比，本文方法能够实现实时执行，并通过策略和框架的提出，增强模型性能。

- **研究贡献:** : 本文实现了两种基于Resnet和LCNN架构的Deepfake音频检测模型，并在ASVspoof 2019数据集上达到了与ASVspoof 2019挑战基线相当的性能。同时，提出了提升这些模型的策略和框架，为通讯平台中实时Deepfake音频检测的发展铺平了道路。

- **研究方法:** : 本文采用了ASVspoof 2019数据集来实现和评估两种Deepfake音频检测模型，并开发了一个跨平台的可执行软件，以支持实时检测。

- **具体表现:** : 在ASVspoof 2019数据集上，所提出的基于Resnet和LCNN架构的模型达到了与ASVspoof 2019挑战基线相当的性能，证明了其在实时通讯场景中的有效性和实用性。

### [安全对齐]《Large Language Model Alignment: A Survey》

 [阅读全文](https://arxiv.org/abs/2309.15025)

大语言模型安全对齐综述