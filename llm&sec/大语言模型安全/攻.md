# 攻

### 《ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs》

#### [阅读全文](http://arxiv.org/abs/2402.11753v1)

- **研究背景:** : 当前大型语言模型（LLMs）的安全性至关重要。尽管已经开发了多种技术如数据过滤和监督式微调来加强LLMs的安全性，但这些技术都假设LLMs的安全对齐仅依赖于语义解释，而这一假设在现实应用中并不成立，导致LLMs存在严重的安全漏洞。

- **实现方法:** : 过去的方法主要依赖于数据过滤和监督式微调来提升LLMs的安全性，但这些方法没有考虑到非语义信息，如ASCII艺术，可能导致的安全问题。本文提出了一种新颖的基于ASCII艺术的越狱攻击方法ArtPrompt，并引入了一个全面的基准测试Vision-in-Text Challenge (ViTC)来评估LLMs识别非纯语义提示的能力。与现有方法不同，ArtPrompt利用LLMs在识别ASCII艺术方面的不足来绕过安全措施，诱导LLMs产生不期望的行为。该方法仅需要对受害LLMs的黑盒访问，因此具有实际应用价值。

- **研究贡献:** : 本文提出了ArtPrompt攻击方法，通过ASCII艺术绕过LLMs的安全措施，并创建了Vision-in-Text Challenge (ViTC)基准测试来评估LLMs处理包含视觉信息的文本的能力。研究表明，即使是最先进的LLMs（如GPT-3.5, GPT-4, Gemini, Claude, 和Llama2）也难以处理ASCII艺术形式的提示。

- **研究方法:** : 本文通过设计ASCII艺术形式的提示，并在五个最先进的LLMs上测试这些提示，来评估LLMs的安全性。通过这种方式，研究者能够揭示LLMs在处理ASCII艺术时的弱点，并据此开发出能够有效绕过LLMs安全措施的ArtPrompt攻击方法。

- **具体表现:** : 通过在五个最先进的LLMs上应用ArtPrompt，研究者展示了该方法能够有效且高效地诱导所有五个LLMs产生不期望的行为。这一结果支持了研究者提出的攻击方法能够成功绕过LLMs的安全措施的目标。

### 《LLM Agents can Autonomously Hack Websites》

#### [阅读全文](https://arxiv.org/html/2402.06664v3)

```commandline
使用大语言模型自动化攻击website
```

### 《Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction》

#### [阅读全文](http://arxiv.org/abs/2402.18104v1)

- **研究背景:** : 近年来，大型语言模型（LLMs）在各种任务中展现出显著的成功，但LLMs的可信度仍是一个未解决的问题。特别是，攻击者可能会设计恶意提示，诱导LLMs生成有害回应。

- **实现方法:** : 过去的方法存在的问题是，它们可能无法有效地绕过LLMs的安全性微调机制。本文提出的DRA（Disguise and Reconstruction Attack）方法与现有方法不同，它通过伪装将有害指令隐藏起来，并促使模型在其完成中重建原始的有害指令。DRA方法很好地解决了这些问题，并且动机明确。

- **研究贡献:** : 本文首次为LLMs安全性提供了理论基础，设计了一种黑盒越狱方法DRA，该方法在多个开源和封闭源模型上进行评估，展示了最先进的越狱成功率和攻击效率。

- **研究方法:** : 本文采用的DRA方法通过伪装和重建攻击来测试LLMs的偏见漏洞，并评估了该方法在不同模型上的有效性。

- **具体表现:** : DRA在GPT-4等LLM聊天机器人上展示了90%的攻击成功率，证明了其方法在实际任务中的高效性，支持了他们的目标。

### 《Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues》

#### [阅读全文](https://arxiv.org/html/2402.09091v2)

```commandline
采取防御姿态，通过法学硕士收集原始恶意查询的线索，来进行prompt攻击越狱。

```

### 《Smishing Dataset I: Phishing SMS Dataset from Smishtank.com》

#### [阅读全文](http://arxiv.org/abs/2402.18430v1)

- **研究背景:** : 随着短信钓鱼攻击（Smishing）成为最常见的社会工程攻击类型之一，相关的smishing数据集却相对缺乏。现有的防御机制面临着新鲜smishing数据集获取困难的挑战，且随着时间推移，smishing活动被关闭，相关攻击信息也随之丢失。因此，研究者和工程师需要不断更新的smishing样本来创建有效的防御措施。

- **实现方法:** : 过去的方法主要依赖于已知的smishing攻击样本来构建防御机制，但这些方法存在问题，如数据集更新不及时，样本覆盖面不广，且难以捕捉到新兴的smishing攻击模式。本文提出的方法与现有方法的不同之处在于，它提供了一个来自smishtank.com的社区来源的smishing数据集，这个数据集包含了最新的smishing样本，并且随着社区的持续贡献，数据集会不断更新。这种方法解决了数据时效性和覆盖面的问题，为研究和工业界提供了一个动态更新的资源。

- **研究贡献:** : 本文的主要贡献是提供了一个包含1090个公开提交的smishing样本的语料库。每条消息都包含了发件人信息、消息正文以及消息中提及的任何品牌信息。当消息中包含URL时，还提供了关于域名的额外信息、VirusTotal的结果以及URL的特征描述。这个开放获取的新鲜smishing数据为学术界和工业界打造针对这一不断演变威胁的强大防御提供了支持。

- **研究方法:** : 本文采用的研究方法是通过分析和整理smishtank.com网站上公开提交的smishing样本，构建了一个详细的smishing数据集。研究团队对每个样本进行了详细的分析，包括发送者信息、消息内容、品牌引用、URL特征和VirusTotal结果等。

- **具体表现:** : 本文并未提及具体的性能评估或实验结果，因为它主要关注于数据集的构建和提供。然而，这个数据集的质量和更新性对于研究社区来说是非常有价值的，它为开发和评估新的smishing检测和防御方法提供了基础。因此，尽管没有直接的性能指标，这个数据集的存在本身就支持了其目标——促进对抗smishing威胁的研究和应用开发。

### 《DevPhish: Exploring Social Engineering in Software Supply Chain Attacks on Developers》

#### [阅读全文](http://arxiv.org/abs/2402.18401v1)

- **研究背景:** : 本文探讨了软件供应链（Software Supply Chain, SSC）中针对软件开发者的社会工程（Social Engineering, SocE）攻击。随着攻击者越来越多地关注利用社会工程技术来潜入系统和破坏组织，特别是在软件开发生命周期（Software Development Life Cycle, SDLC）的关键步骤中，如访问Github仓库、合并代码依赖以及获取拉取请求（Pull Requests, PR）的批准来引入恶意代码。

- **实现方法:** : 文中分析了现有的社会工程攻击方法及其问题，指出过去的方法缺乏针对软件工程师（Software Engineers, SWEs）的特定攻击策略。本文提出的方法与现有方法的不同之处在于，它系统地概述了在软件供应链中针对软件工程师的操纵策略，并通过威胁建模和安全缺口分析来解决这些问题。该方法的提出是基于对现实世界事件和学术文献的广泛分析，因此具有很强的动机。

- **研究贡献:** : 本文的贡献在于全面探索了针对软件工程师的社会工程攻击策略，并提供了一个系统的概述，这对于威胁建模和安全缺口分析非常有益。

- **研究方法:** : 本文采用了文献综述和案例分析的方法，通过分析学术文献和真实世界中的社会工程攻击事件，来系统地总结和呈现软件供应链中的操纵策略。

- **具体表现:** : 文章并未提及具体的性能指标或实验结果，而是侧重于提供对现有和新兴社会工程攻击策略的系统性概述。因此，本文的目标不在于展示方法的性能，而是在于提高对此类攻击的认识和理解，从而支持其目标。

### 《WIPI: A New Web Threat for LLM-Driven Web Agents》

#### [阅读全文](https://arxiv.org/html/2402.16965v1)

```commandline
大语言模型通过agent代理的导致的威胁
```

### 《Tree of Attacks: Jailbreaking Black-Box LLMs Automatically》

#### [阅读全文](https://arxiv.org/html/2312.02119v2)

```commandline
通过攻击树的方法，来搜索最优的越狱方法
```

### 《Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild》

#### [阅读全文](https://arxiv.org/html/2312.02119v2)

```commandline
加州大学伯克利分校人类可兼容人工智能研究中心发表了自然语言处理模型(NLLMs)的对抗性使用相关研究。研究的目的是探讨人们如何利用和攻击自然语言处理模型，并解释其背后的动机和策略。

具体内容包括通过质化方法进行深入研究，采访了28名来自不同背景且从事这类活动的个人，分析了他们的动机、目标、策略，并提出了一个“野外红队理论”，该理论阐释了人们进行自然语言处理模型攻击的原因、目的和方法。

```

### 《Linguistic Obfuscation Attacks and Large Language Model Uncertainty》

#### [阅读全文](https://www.researchgate.net/publication/377814913_Linguistic_Obfuscation_Attacks_and_Large_Language_Model_Uncertainty)

```commandline
德国Amberg-Weiden技术学院学者通过语法解析树和变异规则，设计出更加复杂且难以理解的句子。这种方法可以将句子的语义隐藏在复杂的语法结构之后，从而绕过安全机制，导致模型生成有害或不想要的输出。同时通过增强漏洞、隐藏提示语义、模型预测不确定性、削弱模型处理能力及注意力机制等方式将模型输出不确定性显著增强。

作者还提出了一种在处理前先简化提示的防御机制，以减少模型面临语言混淆攻击的不确定性。这种简化提示的方法可以通过减少语法结构的复杂性和消除过于模糊的表述来降低模型的不确定性，同时使得提示更容易理解和分析。

```

### 《PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models》

#### [阅读全文](https://arxiv.org/html/2402.07867v1)

```commandline
现有研究主要关注提高RAG的准确度或效率，而对其安全性研究较少。宾夕法尼亚大学和武汉大学的研究人员探讨了大语言模型检索增强生成（RAG）的安全性。检索增强生成（RAG）是一种先进技术，旨在减轻这些局限。具体来说，RAG在给定问题时从知识库中检索相关知识以增强LLM的输入。

例如，当知识库包含从维基百科收集的数百万篇文本时，检索到的知识可能是一组与给定问题最语义相似的top-k文本。结果，LLM可以将检索到的知识作为上下文来为给定问题生成答案。本文旨在填补这一空白。特别是，作者提出了PoisonedRAG，一系列针对RAG的知识污染攻击，其中攻击者可以向知识库中注入少量有毒文本，以便LLM为攻击者选择的特定问题生成攻击者选择的特定答案。

作者将知识污染攻击视为一个优化问题，其解决方案是一组有毒文本。根据攻击者对RAG的背景知识（例如，黑盒子和白盒子环境），分别提出了两种解决方案来解决问题。我们在多个基准数据集和LLM上的实验结果显示，当向包含数百万文本的数据库中注入5条针对每个目标问题的有毒文本时，我们的攻击可以实现90%的成功率。我们还评估了最近的防御措施，并发现它们不足以防御我们的攻击，这突显了需要新的防御措施。

```

### 《How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?》

#### [阅读全文](https://arxiv.org/abs/2402.09546)

```commandline
在机器人学和自动化领域，基于LLM的导航系统表现出令人印象深刻的能力，但是在安全性方面还很少受到关注。纽约大学的研究人员探讨了大型语言模型（LLM）在都市户外环境导航系统的安全性问题。本文首次研究了LLM基座导航模型在都市户外环境中的漏洞，这是非常关键的，因为这项技术在自动驾驶、物流和紧急服务中都有广泛应用。

尤其是介绍了一种新颖的导航提示后缀（NPS）攻击，这种攻击通过在原始导航提示后添加基于梯度的后缀来操纵LLM基础的导航模型，从而导致采取错误的行动。研究人员在一个采用各种LLM进行推理的LLM基础导航模型上进行了全面的实验，这个模型使用了Touchdown和Map2Seq街景数据集，在少量学习和微调配置下，结果显示，面对白盒和黑盒攻击，三个指标都出现了显著的性能下降。这些结果突出了NPS攻击的通用性和可转移性，强调了在LLM基础导航系统中增强安全性的必要性。作为一项初步对策，提出了导航提示工程（NPE）防御策略，专注于导航相关的关键词，以减少对抗性后缀的影响。尽管初步发现表明，这种策略增强了导航安全性，但是还需要研究界开发更强的防御方法，有效应对这些系统面临的现实挑战。

```

### 《PAL: Proxy-Guided Black-Box Attack on Large Language Models》

#### [阅读全文](https://arxiv.org/html/2402.09674v1)

```commandline
Chawin Sitawarin等人提出一种针对大型语言模型（LLM）的新型攻击方法，名为PAL。在这项工作中，作者介绍了针对LLM的代理引导攻击（PAL），这是第一个在仅黑盒查询设置中的基于优化的LLM攻击。

根据论文内容，PAL攻击在黑盒设置下取得了显著的成功率，并且相比于现有攻击方法具有明显的优势：

攻击成功率：PAL攻击在25k次查询内可以以74%的成功率对GPT-3.5-Turbo进行“越狱”，远高于现有攻击方法TAP的58%成功率。此外，对难攻的Llama-2-7B模型，其取得了48%的成功率，比现有攻击方法的4%高得多。

成本优势：在黑盒设置下，PAL攻击平均每次成功“越狱” GPT-3.5-Turbo仅需要花费0.88美元，成本仅为现有攻击方法的一半左右。

实用攻击：PAL攻击的关键创新包括使用代理模型的梯度指导攻击，并设计了针对商业LLM API的损失函数计算方法，使攻击成为在真实环境中的第一次基于token的优化攻击。
```

### 《Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents》

#### [阅读全文](https://arxiv.org/pdf/2402.11208.pdf)

```commandline
北京大学、人民大学和腾讯AI实验室的几名研究员研究了大型语言模型（LLM）基础Agents在处理金融、医疗、购物等现实世界应用中的安全性问题。尽管LLM-based agents在各个领域得到了广泛的应用，但其安全性问题目前还尚未得到充分探索。

文章首先提出了一个Agent后门攻击的一般框架，然后对不同形式的Agent后门攻击进行了彻底的分析。具体而言，从最终攻击结果的角度来看，攻击者可以选择操纵最终输出分布，或者仅在中间推理过程中引入恶意行为，同时保持最终输出的正确性。

此外，前者可以根据触发位置分为两个子类别：后门触发器可以隐藏在用户查询中，或者在外部环境返回的中间反馈中。

为了实现针对两个典型Agent任务（网上购物和工具利用）的上述Agent后门攻击的变种，研究人员提出了相应数据污染机制。大量实验表明，LLM-based agents遭受严重后门攻击，迫切需要进一步研究开发针对LLM-based agents后门攻击的防御措施。

```

### 《Stealthy Attack on Large Language Model based Recommendation》

#### [阅读全文](https://arxiv.org/abs/2402.14836)

```commandline
近年来，大型语言模型（LLM）的强大能力极大地推动了推荐系统（RS）的进步。中科院的研究人员揭示了将LLM引入推荐模型中会带来新的安全风险，因为这些模型对项目文本内容的曝光使得它们容易受到攻击。

研究人员证明了攻击者仅仅通过在测试阶段改变项目的文本内容就可以显著提高其曝光度，而无需直接干预模型的训练过程。

此外，这种攻击的特点是隐蔽，因为它不会影响整体的推荐性能，而且对文本的修改微妙，使得用户和平台难以察觉。

研究人员在四个主流的基于LLM的推荐模型上进行了全面的实验，证明攻击方法在效果和隐蔽性上的优越性。本文揭示了对基于LLM的推荐系统存在一个重要的安全风险，并为进一步保护这些系统的研究铺平了道路。

```

### 《InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents》

#### [阅读全文](https://arxiv.org/abs/2403.02691)

- **研究背景:** : 随着大型语言模型（LLMs）被赋予代理身份，它们能够访问工具、执行动作以及与外部内容（如电子邮件或网站）交互。但这也带来了间接提示注入（IPI）攻击的风险，即在LLMs处理的内容中嵌入恶意指令，以操纵代理执行对用户有害的动作。

- **实现方法:** : 过去的方法没有充分考虑到LLMs作为代理时，外部内容可能引发的安全问题。本文提出的InjecAgent基准与现有方法不同，专门用于评估工具集成LLM代理对IPI攻击的脆弱性，并通过测试案例来揭示和缓解这些风险。InjecAgent通过提供大量测试案例，强化了对LLM代理安全性的评估。

- **研究贡献:** : 本文引入了InjecAgent基准，包含1,054个测试案例，涵盖17种用户工具和62种攻击者工具。这些测试案例被分类为两种主要攻击意图：对用户直接伤害和私人数据的泄露。此基准为评估和改进LLM代理的安全性提供了工具。

- **研究方法:** : 本文通过InjecAgent基准对30种不同的LLM代理进行评估，以测试它们对IPI攻击的脆弱性。此外，还研究了在攻击者指令被黑客提示强化的情况下，攻击成功率的变化。

- **具体表现:** : 在评估中，ReAct-prompted GPT-4在24%的情况下容易受到攻击。当攻击者指令被黑客提示强化后，攻击成功率几乎翻倍。这些结果表明LLM代理在安全性方面存在脆弱性，对其广泛部署提出了质疑。

### 《ImgTrojan: Jailbreaking Vision-Language Models with ONE Image》

#### [阅读全文](http://arxiv.org/abs/2403.02910v2)

- **研究背景:** : 本文关注大型语言模型(LLMs)与人类价值观的对齐问题，特别是将其与视觉模块结合使用时的安全性问题，即视觉语言模型(VLMs)的安全隐患。

- **实现方法:** : 过去的方法尚未充分探讨VLMs的安全性问题。本文提出了一种新颖的“越狱攻击”(jailbreaking attack)，旨在绕过VLMs的安全屏障，当用户输入有害指令时执行攻击。与现有方法相比，本文的方法通过在训练数据中加入被投毒的(图像, 文本)数据对，替换原始文本标题为恶意的越狱提示，从而实现攻击。此方法针对提到的问题，分析了毒化比例和可训练参数位置对攻击成功率的影响，并通过设计的评估指标来验证攻击的隐蔽性和成功率。

- **研究贡献:** : 本文提出了一种针对VLMs的越狱攻击方法，并设计了两个评估指标来量化攻击的成功率和隐蔽性。同时提供了一份精心策划的有害指令列表，作为衡量攻击效果的基准，并通过与基线方法的比较来展示本文攻击方法的有效性。

- **研究方法:** : 本文假设在训练数据中包含了被投毒的(图像, 文本)数据对，并通过替换文本标题为恶意的越狱提示来执行攻击。同时分析了毒化比例和可训练参数位置对攻击成功率的影响，并设计了评估指标来量化攻击的成功率和隐蔽性。

- **具体表现:** : 本文通过设计的评估指标来量化攻击的成功率和隐蔽性，并提供了一份有害指令列表作为衡量攻击效果的基准。通过与基线方法的比较，本文展示了攻击方法的有效性，但具体的数值表现未在摘要中提及，因此无法判断其性能是否支持他们的目标。

### 《Human vs. Machine: Language Models and Wargames》

#### [阅读全文](http://arxiv.org/abs/2403.03407v1)

- **研究背景:** : 本文探讨了在军事战略发展和国家对威胁或攻击的响应中，战争游戏的悠久历史。随着人工智能（AI）的出现，承诺提供更好的决策制定和提高军事效能。然而，关于AI系统，特别是大型语言模型（LLMs）与人类相比的行为表现，仍存在争议。

- **实现方法:** : 过去的方法依赖于人类专家进行战争游戏模拟，但这种方法耗时且可能受到个人偏见的影响。本文提出的方法是使用大型语言模型（LLMs）模拟战争游戏中的人类行为，并与107名国家安全专家的实际游戏结果进行比较。这种方法旨在减少人为误差并提高模拟的效率。提出的方法通过与人类专家的决策进行比较，来验证LLMs的有效性和局限性。

- **研究贡献:** : 本文通过与国家安全专家的战争游戏实验比较，揭示了LLMs与人类在模拟美国-中国危机升级情景中的相似性和差异性。研究发现LLMs与人类响应之间有相当程度的一致性，但在定量和定性上也存在显著差异，这提示政策制定者在赋予AI自主权或遵循基于AI的战略建议之前应保持谨慎。

- **研究方法:** : 本文采用了战争游戏实验，其中包括107名国家安全专家在一个虚构的美国-中国情景中进行危机升级的决策模拟。研究者比较了这些人类玩家的响应与LLMs模拟出的响应。

- **具体表现:** : 在模拟的战争游戏任务中，LLMs与人类玩家的响应在一定程度上是一致的，但在定量和定性分析中也显示出了重要差异。尽管LLMs能够在某种程度上模拟专家的决策，但研究结果表明，在实际应用AI进行战略决策之前，仍需谨慎考虑这些差异。这些发现支持了研究的目标，即评估LLMs在高风险决策环境中的潜力和局限性。

### 《Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks》

#### [阅读全文](http://arxiv.org/abs/2403.03792v1)

- **研究背景:** : 本文介绍了一种新型的提示注入攻击方法，称为Neural Exec。与依赖手工制作的字符串（例如，“忽略之前的指令并...”）的已知攻击不同，本研究提出了将执行触发器的创建视为一个可微分搜索问题，并使用基于学习的方法来自主生成它们。

- **实现方法:** : 过去的方法依赖于手工制作的攻击字符串，存在的问题是效果有限且容易被现有的黑名单检测和清洗方法识别。Neural Exec与现有方法的不同之处在于，它通过学习生成触发器，解决了这些问题，并且所提出的方法具有很强的动机性，因为它能够生成形状、属性和功能具有内在灵活性的触发器。

- **研究贡献:** : 本文证明了动机驱动的攻击者可以制造出比当前手工制作的触发器更有效的Neural Exec，并且这些触发器能够在多阶段预处理管道中持续存在，例如在Retrieval-Augmented Generation (RAG)基础的应用中。更关键的是，攻击者可以生成形式和形状大幅偏离已知攻击的触发器，从而规避现有的基于黑名单的检测和清洗方法。

- **研究方法:** : 本文采用了基于学习的方法来进行可微分搜索，自主生成执行触发器。这种方法可以通过优化过程来发现能够绕过预处理和检测机制的有效触发器。

- **具体表现:** : 通过实验，本文的方法在生成能够持续存在于Retrieval-Augmented Generation (RAG)等多阶段预处理管道中的执行触发器方面表现出色。这些触发器的形式和功能的灵活性证明了Neural Exec能够支持其目标，即绕过现有的检测和防御机制。

### 《DeepEclipse: How to Break White-Box DNN-Watermarking Schemes》

#### [阅读全文](http://arxiv.org/abs/2403.03590v1)

- **研究背景:** : 随着深度学习（DL）模型在数字化转型中的关键作用日益凸显，其知识产权保护引起了广泛关注。为此，研究者们开发了不同的水印技术来保护深度神经网络（DNN）不受知识产权侵犯。

- **实现方法:** : 过去的方法主要是白盒水印技术，通过在特定DNN层添加独特签名来修改权重。这些方法存在的问题包括需要知道特定的水印方案或者需要访问底层数据进行进一步的训练和微调。DeepEclipse提出了一种新的统一框架，与现有的白盒水印移除方案显著不同，无需预先知识、额外数据或训练微调即可移除水印。该方法很好地解决了现有方法的问题，并且动机充分。

- **研究贡献:** : 本文提出的DeepEclipse框架能够在不知道水印方案、无需额外数据或训练的情况下，有效移除白盒水印。它降低了水印检测的准确性至随机猜测水平，同时保持了与原始模型相似的准确率。

- **研究方法:** : 本文提出了一系列混淆技术，用于破坏多种白盒水印方案，并通过实验评估了DeepEclipse的有效性。

- **具体表现:** : DeepEclipse在破坏多个白盒水印方案方面表现出色，将水印检测的准确性降低到了随机猜测的水平，同时保持了与原始模型相似的模型准确率。这些性能支持了他们的目标，即展示了一个应对DNN水印保护和移除挑战的有希望的解决方案。

### 《Stealing Part of a Production Language Model》

#### [阅读全文](https://arxiv.org/abs/2403.06634)

```commandline
1、目标和范围：攻击专门针对生产级别的语言模型，如OpenAI的ChatGPT或Google的PaLM-2，这些模型通常作为黑箱服务通过API接口提供。攻击的主要目标是提取模型的嵌入投影层信息。

2、成本效益：该方法成本效率极高，能够以不到20美元的成本提取出OpenAI语言模型的投影矩阵，这一点通过实验确认。

3、揭示隐藏维度：通过这种攻击，研究人员首次确认了黑箱模型具有的隐藏维度大小（例如，1024和2048），这种维度信息对于理解模型的能力和结构至关重要。

4、高效率：该方法不仅成本低廉，而且高效，能够快速提取出模型的关键信息。例如，预计用不到2000美元就能恢复gpt-3.5-turbo模型的完整投影矩阵。

5、高精度：通过这种方法提取的模型信息（如投影矩阵）具有高精度，误差极小。例如，对于OpenAI模型，提取的嵌入层与实际模型之间的平均平方误差非常低（10^-4级别），证明了攻击的精确性。

6、不同模型的适用性：攻击方法在多个不同的模型上进行了测试，包括不仅限于OpenAl的模型。这表明该攻击手段具有一定的普适性，可以应用于多种生产级语言模型。

7、安全和隐私影响：该研究揭示了即使是最尖端的语言模型，也存在被窃取关键信息的风险，这对模型的安全性和使用者的隐私提出了挑战，同时也提示需要更加重视模型的安全防护措施。

```

### 《Exploring Safety Generalization Challenges of Large Language Models via Code》

#### [阅读全文](http://arxiv.org/abs/2403.07865v1)

- **研究背景:** : 大型语言模型（LLMs）在自然语言处理方面取得了显著进展，但同时也引发了对其潜在滥用的担忧。现有的安全增强策略如监督式微调和基于人类反馈的强化学习主要关注自然语言，可能无法泛化到其他领域。

- **实现方法:** : 本文提出了一个名为CodeAttack的框架，它将自然语言输入转换为代码输入，为测试LLMs的安全泛化提供了新的环境。与现有方法相比，CodeAttack揭示了GPT-4、Claude-2和Llama-2系列等先进LLMs在代码输入方面的共同安全漏洞，这些模型的安全防护措施超过80%的时间被绕过。此外，CodeAttack与自然语言之间的分布差距越大，安全泛化能力越弱。该方法通过提供一个新的测试环境，突出了代码领域的新安全风险，并激励了开发与LLMs的代码能力相匹配的更健壮的安全对齐算法。

- **研究贡献:** : 本文通过CodeAttack框架揭示了LLMs在代码输入方面的安全漏洞，并展示了模型安全防护措施的不足。研究还发现，自然语言输入与代码输入之间的分布差异越大，模型的安全泛化能力越差。

- **研究方法:** : 本文采用了CodeAttack框架，通过将自然语言输入转换为代码输入，测试了GPT-4、Claude-2和Llama-2系列等LLMs的安全性。研究方法包括使用数据结构编码自然语言输入和使用不太流行的编程语言来增加分布差异。

- **具体表现:** : 通过CodeAttack测试，发现所有模型的安全防护措施有超过80%的时间被绕过。这一结果支持了研究目标，即现有LLMs在代码领域存在安全风险，需要更强大的安全对齐算法。

### 《Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models》

#### [阅读全文](http://arxiv.org/abs/2403.07654v1)

- **研究背景:** : 随着序列到序列(sequence-to-sequence)相关性模型如monoT5的发展，它们能够通过交叉编码有效捕捉查询和文档之间的复杂文本交互。然而，这些模型使用自然语言令牌作为提示，如Query、Document和Relevant，这为恶意文档通过提示注入（prompt injection）操纵相关性评分提供了攻击途径。

- **实现方法:** : 过去的方法主要依赖于传统的文本检索技术，如BM25，它们对于自然语言处理的攻击不太敏感。但是，基于序列到序列的模型由于使用了自然语言提示，容易受到攻击。本文提出了一种新的分析方法，通过手工构造的模板和基于大型语言模型(LLM)的文档重写，来评估这种攻击对多个现有相关性模型的影响。与传统方法相比，本文的方法更加关注模型的安全性和鲁棒性。

- **研究贡献:** : 本文分析了查询独立的提示注入对多个序列到序列相关性模型的影响，并发现这些模型容易被恶意文档操纵，而传统的词汇模型如BM25则不受影响。此外，即使是不依赖自然语言提示令牌的编码器模型，也在一定程度上受到了攻击的影响。

- **研究方法:** : 本文通过在TREC Deep Learning track上的实验，使用手工构造的模板和基于大型语言模型的文档重写方法，来分析不同序列到序列相关性模型对于恶意文档的敏感性。

- **具体表现:** : 实验结果显示，通过提示注入，攻击者可以轻易操纵不同的序列到序列相关性模型，而BM25作为典型的词汇模型则不受影响。这些攻击同样影响了仅基于编码器的相关性模型，尽管影响程度较小。这些发现支持了本文的目标，即揭示并分析序列到序列相关性模型在面对恶意攻击时的脆弱性。

### 《Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology》

#### [阅读全文](https://arxiv.org/html/2402.15690v1)

```commandline
国防科技大学的研究人员从认知心理学的角度解释了大模型“越狱现象”，根据文中的内容，作者提出的基于自我感知理论的越狱方法FITD(Foot-in-the-Door)的工作机制如下：

1、FITD利用自我感知理论，即个体往往会通过其行为来形成态度和价值观。

2、FITD分阶段向大型语言模型提出问题，先从较小的问题开始，逐步提出更敏感的问题。

3、当模型接受并回答一个较小的问题后，模型形成了与该问题一致的态度，从而更容易接受并回答更敏感的问题。

4、若模型拒绝问题，FITD会对问题进行拆分，递归地提供问题的不同部分给模型，引导其回答。

5、FITD使用逐渐提出问题和递归拆分问题的方式，让模型在对话中逐渐接受越来越多敏感的内容，最终成功越狱。

```

### [多模态攻击]《FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs》

#### [阅读全文](http://arxiv.org/abs/2403.13507v2)

- **研究背景:** : 随着视频基础的大型语言模型（LLMs）在性能上取得显著进展，它们面临的对抗性威胁尚未得到充分探索。本文旨在填补这一研究空白。

- **实现方法:** : 以往的方法没有专门针对视频基础的LLMs设计对抗攻击，存在的问题包括对抗样本的生成不够高效或不适用于多模态场景。本文提出的FMM-Attack方法通过在视频中少量帧上制造基于流的多模态对抗扰动，与现有方法相比，能更有效地针对视频LLMs进行攻击，并解决了以往方法的局限性。该方法的动机明确，旨在提高模型的鲁棒性和安全性。

- **研究贡献:** : 本文提出了首个专为视频基础的LLMs设计的对抗攻击方法FMM-Attack，并通过实验展示了其有效性。此外，本文的研究还启发了对多模态鲁棒性和跨不同模态的安全相关特征对齐的进一步理解。

- **研究方法:** : 本文采用了一种流基的多模态对抗攻击技术，通过在视频的部分帧上添加不易察觉的扰动，来诱导视频LLMs产生错误的答案或幻觉。

- **具体表现:** : 通过在视频LLMs上的广泛实验，FMM-Attack能够有效地导致模型生成不正确的答案，甚至产生语义上的混乱。这些表现支持了本文的目标，即揭示并提高视频LLMs在面对对抗性攻击时的鲁棒性。

### [后门攻击]《BadEdit: Backdooring large language models by model editing》

#### [阅读全文](http://arxiv.org/abs/2403.13355v1)

- **研究背景:** : 当前主流的后门攻击方法通常需要大量的调整数据进行投毒，这限制了它们的实用性，并可能在应用于大型语言模型（LLMs）时降低整体性能。

- **实现方法:** : 过去的方法需要大量的投毒数据，存在实用性差和可能损害模型性能的问题。BadEdit将后门注入定义为一种轻量级的知识编辑问题，通过直接修改LLMs参数来植入后门，无需大量数据，且对模型性能影响小。与现有方法相比，BadEdit更实用，需要的数据集更小（15个样本），只调整部分参数以提高效率，并确保模型主要性能不受影响，即使在后续的微调或指令调整后，后门依然稳固。

- **研究贡献:** : BadEdit框架首次将后门注入问题转化为轻量级的知识编辑问题，并且在保持模型对正常输入的性能的同时，能够高效地攻击预训练的大型语言模型，具有高达100%的成功率。

- **研究方法:** : BadEdit通过高效的编辑技术直接改变LLMs的参数，实现后门的注入。该方法仅需少量数据集，并且通过调整参数子集来减少时间消耗。

- **具体表现:** : 在预训练的大型语言模型上，BadEdit展示了高效的攻击能力，成功率可达100%，同时保持了模型对良性输入的性能不受损害。这些性能表现支持了他们的研究目标。

### [越狱攻击]《EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2403.12171v1)

- **研究背景:** : 本文针对大型语言模型（LLMs）的安全漏洞，探讨了越狱攻击（jailbreak attacks）的重要性，这类攻击旨在绕过安全防护措施，获取被禁止的输出。

- **实现方法:** : 过去的越狱方法各不相同，缺乏一个标准的实现框架，限制了对LLMs的全面安全评估。本文提出的EasyJailbreak框架通过Selector、Mutator、Constraint和Evaluator四个组件，简化了越狱攻击的构建和评估，使研究人员能够轻松地从新旧组件组合中构建攻击。

- **研究贡献:** : EasyJailbreak支持11种不同的越狱方法，有助于广泛验证LLMs的安全性。此外，本文还提供了丰富的研究资源，包括一个网页平台、发布在PyPI的软件包、演示视频和实验输出。

- **研究方法:** : 本文采用的EasyJailbreak框架采用模块化设计，通过组合不同的Selector、Mutator、Constraint和Evaluator组件来构建越狱攻击，并对LLMs进行安全评估。

- **具体表现:** : 在10种不同的LLMs上进行验证，平均遭受攻击的概率为60%。即使是高级模型如GPT-3.5-Turbo和GPT-4，其平均攻击成功率（ASR）分别为57%和33%。这些性能结果支持了EasyJailbreak框架能够有效地揭示和评估LLMs的安全漏洞。

### [对抗攻击]《SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator》

#### [阅读全文](http://arxiv.org/abs/2403.11833v1)

- **研究背景:** : 机器学习模型容易受到恶意制造的对抗性样本（Adversarial Examples, AEs）的攻击。在自然语言处理（Natural Language Processing, NLP）领域，开发高质量AEs的模型发展较慢。

- **实现方法:** : 过去的方法存在问题，如生成的AEs在语义和语法上与原文不一致，且对抗攻击的隐蔽性不足。本文提出的SSCAE模型通过动态阈值和局部贪心搜索生成高质量的AEs，与现有方法相比，更注重语义一致性和语法要求，同时减少查询次数和保持相似的扰动率。

- **研究贡献:** : SSCAE模型在保持语义一致性和语法要求的同时，减少了查询次数，并且在所有实验中性能优于现有模型。

- **研究方法:** : SSCAE模型首先识别重要词汇，使用掩码语言模型（Masked Language Model）生成初步替换集，然后利用两个著名的语言模型评估这些替换集的语义和语法特性，引入动态阈值捕获更有效的扰动，并通过局部贪心搜索生成高质量的AEs。

- **具体表现:** : SSCAE模型通过15项比较实验和广泛的参数优化敏感性分析，证明了其有效性和优越性。在所有实验中，SSCAE的性能均优于现有模型，同时保持了更高的语义一致性，查询次数更少，扰动率相当。

### [图对抗攻击]《Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks》

#### [阅读全文](http://arxiv.org/abs/2403.11830v1)

- **研究背景:** : 本文研究了基于图神经网络（Graph Neural Networks, GNN）的网络入侵检测系统（Network Intrusion Detection Systems, NIDS）面临的对抗性攻击问题。随着机器学习（Machine Learning, ML）算法在NIDS中的广泛应用，其对抗性攻击的脆弱性也逐渐暴露出来。

- **实现方法:** : 过去的方法主要集中在传统的ML算法上，但这些算法容易受到基于特征的对抗性攻击。相比之下，GNN基于网络结构模式提高了检测的鲁棒性。然而，GNN引入了新的风险。本文首次提出了针对GNN的网络入侵检测的对抗性攻击形式化，并模拟了攻击者在现实世界场景中需要考虑的问题空间约束。提出的方法通过考虑结构性攻击的可行性，解决了传统方法的问题，并且有很好的动机。

- **研究贡献:** : 本文的贡献在于提出了针对GNN的NIDS的对抗性攻击的首次形式化，明确了现实世界攻击的问题空间约束，并通过实验验证了GNN模型对传统特征攻击的鲁棒性，同时揭示了其对结构性攻击的脆弱性。

- **研究方法:** : 本文通过建立模型和进行广泛的实验来验证提出的攻击方法。实验包括针对最新的GNN-based NIDS发起提出的结构性攻击，并评估其对检测性能的影响。

- **具体表现:** : 通过实验，本文发现GNN模型对传统的基于特征的对抗性攻击具有较高的鲁棒性，但对于结构性攻击则存在脆弱性。尽管没有提供具体的数值，但实验结果支持了本文的目标，即展示GNN在NIDS中对抗性攻击的新风险。

### [后门攻击]《Invisible Backdoor Attack Through Singular Value Decomposition》

#### [阅读全文](http://arxiv.org/abs/2403.13018v1)

- **研究背景:** : 随着深度学习在各个领域的广泛应用，其安全性问题日益受到关注。其中，后门攻击对深度神经网络（DNNs）构成了严重的安全威胁。

- **实现方法:** : 过去的方法主要考虑在空间域中实现不可见性，导致最近的防御方法容易检测出生成的有毒图像。本文提出的DEBA方法利用奇异值分解（SVD）的数学特性，在训练阶段将不可察觉的后门嵌入模型中，通过替换触发图像的次要特征来保证攻击的有效性，解决了现有方法的问题，并且具有很好的动机。

- **研究贡献:** : 本文提出了一种新型的不可见后门攻击方法DEBA，通过实验评估证明了其在保持高感知质量和高攻击成功率的同时，能够有效抵抗现有的防御措施。

- **研究方法:** : 本文首先对图像进行奇异值分解（SVD），然后用干净图像的次要特征替换触发图像的次要特征，使用它们作为触发条件，确保攻击的有效性。

- **具体表现:** : DEBA在保持被毒化图像的高感知质量和高攻击成功率的同时，能够显著逃避和抵抗现有的防御措施，支持了其目标。

### [自动化红队]《Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts》

#### [阅读全文](https://arxiv.org/pdf/2402.16822.pdf)

```commandline
这是一种生成多样化对抗性提示集合的新颖方法。 Rainbow Teaming 将对抗性提示生成视为质量多样性问题，并使用开放式搜索来生成既有效又多样化的提示。它可以发现模型在广泛领域的漏洞，在本文中包括安全、问答和网络安全。我们还证明，对 Rainbow Teaming 生成的合成数据进行微调可以提高最先进的 LLM 的安全性，而不会损害他们的一般能力和帮助性，从而为开放式自我完善铺平道路。

```

