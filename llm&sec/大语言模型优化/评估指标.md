# 评估指标

### 《Exploring Advanced Methodologies in Security Evaluation for LLMs》

#### [阅读全文](http://arxiv.org/abs/2402.17970v1)

- **研究背景:** : 本文探讨了大型语言模型（LLMs）的安全性评估方法。随着LLMs在处理复杂语言模式和生成连贯文本、图像、音频和视频方面的能力增强，以及它们在特定任务上的微调能力，LLMs在商业化应用中得到了广泛使用。然而，LLMs的快速发展也引发了学术界对安全性和伦理问题的关注，这突显了在LLMs的开发和部署过程中进行安全性评估研究的必要性。

- **实现方法:** : 过去几年中，针对大规模模型的安全性评估已有大量研究。但这些方法存在一些问题，如评估标准不一、缺乏统一的评估框架等。本文提出的方法与现有方法的不同之处在于，它提供了一种全面分析常用评估指标、先进评估框架和LLMs常规评估流程的深入回顾，并讨论了LLMs安全性评估的未来方向。提出的方法针对上述问题进行了解决，并且有充分的动机。

- **研究贡献:** : 本文的贡献在于对最新进展进行了深入的回顾，分析了LLMs安全性评估中常用的评估指标、框架和流程，并对未来的研究方向提出了讨论。

- **研究方法:** : 本文采用了文献综述的研究方法，通过分析和总结现有的研究成果，来探讨LLMs的安全性评估方法。

- **具体表现:** : 文中并未提及具体的任务和性能表现，而是侧重于评估方法的综述和讨论，因此无法提供具体的性能支持其目标的数据。

### 《Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence》

#### [阅读全文](https://arxiv.org/pdf/2402.09880.pdf)

```
Timothy R. McIntosh等人批评了23个最先进的语言模型基准（LLM），通过功能和安全两大支柱，以及人员、过程和技术三个维度的全新统一评估框架进行审视。研究发现这些基准存在重大局限性，包括偏见、难以衡量真实推理能力、适应性问题、实现不一致性、提示工程复杂性、评估者多样性以及在综合评估中忽视文化和意识形态规范等问题。本文强调，鉴于人工智能（AI）的进步，迫切需要制定标准化的方法、明确的法规和伦理指南。

研究倡导从静态基准转向动态行为分析，以准确捕捉语言模型的复杂行为和潜在风险。文章指出，在LLM评估方法论上需要根本性的转变，强调了协同合作对于开发新基准以及提升AI向社会赋能程度的重要性。
```

### 《PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion》

#### [阅读全文](http://arxiv.org/abs/2403.03788v1)

- **研究背景:** : 由于人们对大型语言模型（LLMs）在完成用户指令方面的依赖日益增加，理解它们在现实世界复杂任务完成中的鲁棒性变得至关重要。

- **实现方法:** : 过去的方法没有充分考虑到用户指令的多样性和软件版本的变化。本文提出的PowerPoint任务完成鲁棒性基准（PPTC-R）通过构建对抗性用户指令和变化API数量来模拟不同的软件版本，从而评估LLMs在不同挑战下的鲁棒性。与现有方法相比，PPTC-R更全面地评估了LLMs在多语言和软件版本更新环境下的性能。

- **研究贡献:** : 本文提出了一个新的基准PPTC-R，用于评估LLMs在PowerPoint任务完成中的鲁棒性，并对3个闭源和4个开源LLMs进行了测试。研究发现GPT-4在多语言和版本更新设置中表现最佳，但所有LLMs在面对多重挑战时鲁棒性下降。此外，本文还分析了LLMs的鲁棒性行为和错误原因，为研究人员提供了宝贵的见解。

- **研究方法:** : 本文通过攻击用户指令的句子、语义和多语言层面来构建对抗性用户指令，并通过改变提供的API数量来模拟不同版本的软件环境，从而测试LLMs的鲁棒性。

- **具体表现:** : 在PPTC-R基准测试中，GPT-4展现了最高的性能和较强的鲁棒性，尤其是在版本更新和多语言设置中。然而，当LLMs同时面对多重挑战（例如，多轮）时，它们的鲁棒性会显著下降，导致性能大幅度降低。这些发现支持了研究目标，即评估并提高LLMs在任务完成中的鲁棒性。

### 《German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset》

#### [阅读全文](http://arxiv.org/abs/2403.03750v1)

- **研究背景:** : 大型语言模型（LLMs）在自然语言处理任务上取得了显著进展，但在自动文本摘要生成时仍会出现信息“幻觉”问题，即生成的摘要与源文档内容不一致。

- **实现方法:** : 过去的研究主要集中在英语摘要的幻觉检测，而且现有的多语言方法缺乏德语数据。本文提出了一个新的方法，即absinth数据集，用于检测德语新闻摘要中的幻觉，并探索了最新开源的大型语言模型在这一任务上的表现，包括微调和上下文学习设置。

- **研究贡献:** : 本文提供了absinth数据集，这是一个手动注释的数据集，用于检测德语新闻摘要中的幻觉，以促进德语幻觉检测的进一步研究。

- **研究方法:** : 本文探索了新的开源大型语言模型在幻觉检测任务上的能力，包括在微调和上下文学习环境中的应用。

- **具体表现:** : 本文没有提供具体的性能指标，但通过引入absinth数据集和评估大型语言模型在该数据集上的表现，支持了其目标，即提高德语新闻摘要的一致性检测和生成的质量。

### 《Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem》

#### [阅读全文](http://arxiv.org/abs/2403.03558v1)

- **研究背景:** : 本文探讨了大型语言模型（LLMs）在自然语言处理（NLP）任务中的表现，尤其是它们在面对含糊不清的上下文时容易产生不可靠猜测，即“幻觉”现象。

- **实现方法:** : 过去的方法没有专门针对无法回答的数学文字问题（MWP）来评估LLMs的幻觉现象。本文提出了一种新的评估方法，通过创建一个名为Unanswerable Math Word Problem (UMWP)的数据集，包含5200个问题，分为五个类别。与现有方法相比，本文的方法通过结合文本相似性和数学表达式检测来判断LLMs是否认为问题无法回答，更有效地解决了幻觉问题。

- **研究贡献:** : 本文的贡献在于开发了UMWP数据集，并提出了一种新的评估LLMs幻觉现象的方法。此外，通过对31个LLMs（包括GPT-3, InstructGPT, LLaMA, 和Claude）的广泛实验，证明了上下文学习和带有人类反馈的强化学习（RLHF）训练能显著提高模型避免幻觉的能力。

- **研究方法:** : 本文采用的研究方法包括开发UMWP数据集，结合文本相似性和数学表达式检测的评估方法，以及对多个LLMs进行的实验评估。

- **具体表现:** : 在UMWP数据集上的实验结果显示，通过上下文学习和RLHF训练的LLMs在避免幻觉方面表现更好。这些结果支持了使用MWP作为评估幻觉的可靠和有效方法的目标。