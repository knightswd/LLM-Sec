# 微调

### [lora微调改进]《Improving LoRA in Privacy-preserving Federated Learning》

#### [阅读全文](http://arxiv.org/abs/2403.12313v1)

- **研究背景:** : 本文研究了在隐私保护的联邦学习(FL)环境中，如何改进低秩适应(LoRA)方法以提高预训练语言模型的性能和计算效率。

- **实现方法:** : 过去的方法是通过在每个冻结的预训练模型模块上注入两个可训练的低秩分解矩阵的乘积来进行LoRA微调。但在隐私保护的FL环境中，由于数据异质性、多步本地更新的影响、差分隐私(DP)噪声的放大以及对超参数的敏感性，LoRA可能变得不稳定。本文提出了一种高效有效的LoRA版本，即联邦冻结LoRA(Federated Freeze A LoRA, FFA-LoRA)，通过固定随机初始化的非零矩阵，仅微调零初始化矩阵，解决了这些问题，并且理论和实践上都有所支持。

- **研究贡献:** : 本文提出的FFA-LoRA方法在隐私保护的FL环境中提供了更一致的性能，并且在各种FL任务中相比于传统LoRA具有更好的计算效率，同时减半了联邦微调大型语言模型(LLMs)的通信成本。

- **研究方法:** : 本文采用的核心思想是在联邦学习中固定随机初始化的非零矩阵，只对零初始化的矩阵进行微调，以此来减少因数据异质性和DP噪声放大导致的不稳定性，并减少超参数的敏感性。

- **具体表现:** : 通过在不同的FL任务上的实验，FFA-LoRA展示了与传统LoRA相比更加一致的性能，并且在计算效率上有所提高。这些性能支持了FFA-LoRA在隐私保护的FL环境中的应用目标。
