### 《Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning》

#### [阅读全文](http://arxiv.org/abs/2402.12177v1)

- **研究背景:** : Retrieval Augmented Generation (RAG) 在减少大型语言模型（Large Language Models, LLMs）产生的幻觉方面表现出色。但是，预训练的嵌入模型在特定领域知识中可能表现不佳，需要进行微调。

- **实现方法:** : 以往的方法依赖于对预训练嵌入模型的直接微调，但在只有黑盒模型可用的情况下存在局限性。本文提出的Model augmented fine-tuning (Mafin) 方法通过增加一个可训练的嵌入模型来增强黑盒嵌入模型。与现有方法相比，Mafin 不需要对原始黑盒模型进行修改，只需训练一个小型的增强模型即可解决特定领域知识的应用问题。

- **研究贡献:** : 本文提出了一种新颖的Mafin方法，能够有效提升黑盒嵌入模型的性能，并且只需要训练一个小型的增强模型。此外，本文还在标记和未标记的数据集上验证了该方法的有效性和广泛适用性。

- **研究方法:** : 本文采用了一种模型增强微调的方法，通过在黑盒嵌入模型的基础上增加一个可训练的嵌入模型来进行微调，从而提升模型对特定领域知识的理解和应用能力。

- **具体表现:** : 通过在不同的数据集上进行实验，Mafin 方法在提升黑盒嵌入模型性能方面取得了显著成效。实验结果支持了Mafin方法能够有效增强黑盒模型的性能，且仅需训练小型模型的目标。

### 《Can LLMs Compute with Reasons?》

#### [阅读全文](http://arxiv.org/abs/2402.12080v1)

- **研究背景:** : 大型语言模型(LLMs)在处理复杂数学任务时常常遇到困难，由于依赖统计模式，容易“幻觉”出错误答案。在平均水平的小型语言模型(SLMs)中，由于上下文和训练数据的限制，这一问题更为突出。

- **实现方法:** : 过去的方法依赖于大型语言模型的统计模式，存在问题是容易产生错误答案且逻辑推理能力有限。本文提出的方法是采用分布式网络的SLMs进行“归纳学习”，通过错误驱动学习和提示整合来提升SLMs的推理能力。与现有方法相比，本方法旨在使SLMs接近高参数模型所达到的逻辑应用水平，解决了以往方法的问题，并且动机充分。

- **研究贡献:** : 本文提出了一种新颖的框架，通过分布式网络的SLMs和归纳学习方法，提高了SLMs的逻辑推理能力，为缩小人类与LLMs之间的逻辑差距铺平了道路。

- **研究方法:** : 本文采用的研究方法是构建一个SLMs的分布式网络，并实施错误驱动学习和提示整合策略，以此来提升模型的推理能力。

- **具体表现:** : 文中没有提供具体的任务和性能数据，因此无法判断所提方法是否支持其目标。

### [对话幻觉]《Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations》

#### [阅读全文](http://arxiv.org/abs/2402.11770v2)

- **研究背景:** : 本文介绍了如何使用预训练的大型语言模型（LLM）生成基于内容的多轮问答对话。

- **实现方法:** : 过去的方法存在问题，如对文档的依赖不足导致生成的对话内容出现幻觉现象。本文提出的结构化思维链（SCoT）提示方法通过状态机中的多个状态将复杂任务分解，每个状态使用独特的提示集和工具来增强生成过程。与现有方法相比，SCoT方法通过专门的状态减少幻觉，提高了对文档内容的忠实度。

- **研究贡献:** : SCoT提示方法显著提高了生成对话的质量，增加了对文档的忠实度高达16.8%。此外，使用少量的维基百科数据作为种子示例生成的对话数据能够训练出强大的问答对话代理，在跨域评估中观察到最高13.9%的性能提升。

- **研究方法:** : 本文采用的是一种结构化的状态机方法，通过不同的状态执行子任务，如内容阅读和话语生成，并使用提示和工具来辅助生成过程。

- **具体表现:** : 在开放域对话生成任务中，使用SCoT方法生成的对话数据作为训练材料，能够在跨域评估中相比增加了目标域金标准数据的情况下，提高最高13.9%的性能。这表明SCoT方法能够有效支持其目标，即提高问答对话代理的生成质量和准确性。

### 《Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2402.18099v1)

- **研究背景:** : 本文研究的背景是大型语言模型（LLMs）在特定知识上的行为修改，尤其是在医疗领域，准确性至关重要，模型的幻觉问题不容忽视。

- **实现方法:** : 过去的方法存在专业性和复杂性不足的问题，无法有效处理医疗知识。本文提出的MedLaSA方法通过因果追踪确定知识在神经元中的精确位置，并在LLMs的密集层中引入可扩展的适配器，根据特定知识分配缩放值，从而解决了这些问题。

- **研究贡献:** : 本文的贡献在于提出了两项针对医疗领域的模型编辑研究：直接编辑医疗事实知识和编辑对事实的解释，并开发了MedLaSA策略，建立了两个基准数据集和一系列挑战性的综合评价指标。

- **研究方法:** : 本文采用的研究方法是Layer-wise Scalable Adapter策略，通过因果追踪和可扩展适配器对LLMs进行精确的层级编辑，以及通过建立的基准数据集和评价指标来评估编辑效果。

- **具体表现:** : 通过在医疗LLMs上的广泛实验，MedLaSA展示了其编辑效率，能够在不影响未编辑的相关知识的情况下，对特定知识进行有效修改。这些性能支持了他们的目标，即提高LLMs在医疗领域的应用准确性和可靠性。

### 《Corpus-Steered Query Expansion with Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2402.18031v1)

- **研究背景:** : 当前研究表明，大型语言模型（LLMs）生成的查询扩展可以显著提升信息检索系统的性能，通过生成假设性文档来回答查询。但是，由于LLMs固有知识的限制，扩展与检索语料库之间的不一致导致了幻觉和过时信息等问题。

- **实现方法:** : 过去的方法依赖于LLMs生成的查询扩展，但存在与检索语料库不对齐的问题。本文提出的方法，即Corpus-Steered Query Expansion (CSQE)，受到伪相关反馈（PRF）的启发，通过利用LLMs评估相关性的能力，系统地识别初步检索文档中的关键句子。这些来自语料库的文本随后被用来与LLMs知识强化的扩展一起扩展查询，改善了查询与目标文档之间的相关性预测。与现有方法相比，CSQE更好地解决了信息不对齐的问题，并且无需任何训练即可展现出强大的性能。

- **研究贡献:** : 本文提出了一种新的查询扩展方法CSQE，它结合了语料库中的知识和LLMs的强大能力，以改善信息检索的准确性。此外，CSQE特别适用于LLMs缺乏知识的查询，能够在没有训练的情况下提供强大的性能。

- **研究方法:** : CSQE首先使用LLMs评估初步检索文档中句子的相关性，识别出关键句子。然后，将这些句子与LLMs生成的扩展结合，形成新的查询扩展。这种方法利用了LLMs的语言理解能力和语料库中的实际知识，以提高查询的相关性。

- **具体表现:** : 通过广泛的实验，CSQE在不同的信息检索任务上展现了强大的性能，特别是在LLMs缺乏知识的查询上。尽管文档没有提供具体的数值表现，但实验结果支持了CSQE能够改善查询与目标文档之间的相关性预测的目标。

### 《Retrieval-Augmented Thought Process as Sequential Decision Making》

#### [阅读全文](https://arxiv.org/abs/2402.07812)

```
Thomas Pouplin等人提出了一种新的方法，名为“检索增强思维过程”（Retrieval-Augmented Thought Process，简称RATP），用于改善大型语言模型（Large Language Models，简称LLMs）的决策过程。众所周知，虽然LLMs表现出强大的能力，但仍存在一些挑战，如隐私问题、产生虚构内容的风险以及处理长上下文困难等。RATP通过将LLMs的思维生成过程视为一个多步骤的决策过程来应对这些挑战，并利用蒙特卡洛树搜索（Monte-Carlo Tree Search）和Q值估计器来优化这一过程。特别是在处理包含隐私数据的问答任务时，RATP在现有基于检索增强的语言模型上实现了50%的改进，这对于让大语言模型遵守道德和安全的限制具有重要意义
```

### 《KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents》

#### [阅读全文](http://arxiv.org/abs/2403.03008v1)

- **研究背景:** : 大型语言模型（LLMs）在复杂推理任务中展现出巨大潜力，但在生成可执行动作与环境交互时面临挑战，主要是因为缺乏内置的动作知识，导致规划幻觉问题。

- **实现方法:** : 过去的方法没有有效地引导语言代理在任务解决过程中的规划轨迹。KnowAgent通过引入一个动作知识库和知识自学习策略，限制规划过程中的动作路径，与现有方法相比，能够合理地合成规划轨迹，提高规划性能。

- **研究贡献:** : KnowAgent提出了一种新颖的方法，通过整合显式动作知识来增强LLMs的规划能力。实验结果表明，KnowAgent在HotpotQA和ALFWorld任务上，基于不同的骨干模型，能够达到与现有基线相当或更优的性能，并有效减少规划幻觉。

- **研究方法:** : 本文提出了KnowAgent，它使用动作知识库和知识自学习策略来指导语言代理的规划过程，以生成更合理的规划轨迹。

- **具体表现:** : 在HotpotQA和ALFWorld任务上，KnowAgent基于不同的骨干模型取得了可比或更优的性能，支持了其减少规划幻觉并提高规划性能的目标。

### 《Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations》

#### [阅读全文](http://arxiv.org/abs/2403.03008v1)

- **研究背景:** : 在个性化教育时代，为学习推荐提供易于理解的解释对于增强学习者对推荐学习内容的理解和参与度具有重要价值。

- **实现方法:** : 过去的方法依赖于大型语言模型（LLMs）和生成性AI来生成类似人类的解释，但在教育这样一个敏感领域，它们的精确度仍然不尽人意。本文提出的方法是利用知识图谱（Knowledge Graphs, KG）作为事实背景的来源，为LLM提示提供支持，减少模型幻觉的风险，防止错误或不精确的信息，同时保持应用意图的学习背景。与现有方法相比，本文提出的方法通过利用知识图谱的语义关系来提供关于学习推荐的筛选知识，并且在提示工程阶段加入了领域专家，确保解释包含对学习者有关的信息。

- **研究贡献:** : 本文的贡献在于提出了一种结合知识图谱和LLM的方法来生成学习推荐的解释，并通过领域专家的参与来提高解释的相关性和准确性。

- **研究方法:** : 本文采用的研究方法包括设计文本模板作为解释，由LLM填充和完成，并通过Rouge-N和Rouge-L指标进行量化评估，同时也进行了专家和学习者的定性评估。

- **具体表现:** : 通过本文方法生成的解释与仅由GPT模型生成的解释相比，显示出提高的召回率和精确度，并且在最终学习解释中生成不精确信息的风险大大降低。这些性能结果支持了他们的目标。

### 《In Search of Truth: An Interrogation Approach to Hallucination Detection》

#### [阅读全文](http://arxiv.org/abs/2403.02889v1)

- **研究背景:** : 本文探讨了大型语言模型（LLMs）在日常生活中应用受限的问题，特别是模型产生的幻觉现象，即LLMs创造出听起来合理但与事实真相背离的答案。

- **实现方法:** : 过去的方法存在一定问题，如依赖外部知识库、准确性不足等。本文提出了一种新的方法，不依赖外部知识，通过一种审问式的方法来检测LLMs的幻觉现象，与现有方法相比，提高了检测的准确性和可靠性。

- **研究贡献:** : 本文提出了一种新颖的幻觉检测方法，并通过多个数据集和LLMs（包括Llama-2）的广泛评估，证明了该方法的有效性。

- **研究方法:** : 本文采用了一种审问式的方法来检测LLMs的幻觉现象，通过对模型的回答进行质疑，以判断其是否产生了幻觉。

- **具体表现:** : 在特定实验中，Llama-2模型的幻觉率高达62%，而本文提出的方法在不依赖外部知识的情况下，达到了87%的平衡准确率（B-ACC），支持了研究目标的实现。

### 《Privacy-Aware Semantic Cache for Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2403.02694v1)

- **研究背景:** : 大型语言模型（LLMs）如ChatGPT、Google Bard、Claude和Llama 2在自然语言处理和搜索引擎领域带来了革命性的变化，但它们的计算成本极高。例如，GPT-3包含1750亿个参数，推理过程也需要数十亿次浮点运算。缓存是减少LLMs重复查询推理成本的一种方法，但现有的缓存方法无法识别查询之间的语义相似性，导致错误的命中率和未命中率。

- **实现方法:** : 本文提出了MeanCache，一种能够识别语义相似查询的LLMs语义缓存，通过MeanCache，可以从本地缓存中检索用户语义相似的查询响应，从而减少成本、减轻服务提供商的负载和环境影响。MeanCache利用联合学习（Federated Learning, FL）在不侵犯隐私的情况下，通过众多用户的分布式协作训练查询相似性模型。通过在每个用户设备中放置本地缓存并使用FL，MeanCache降低了延迟和成本，提高了模型性能，显著降低了缓存错误命中率。与现有方法相比，MeanCache更加注重隐私保护和语义理解。

- **研究贡献:** : 本文的MeanCache在语义缓存命中与未命中决策中，相较于GPTCache，F-score提高了约17%，精确度提高了20%。此外，MeanCache减少了83%的存储需求，并将语义缓存命中与未命中决策的速度提高了11%，在性能上超越了GPTCache。

- **研究方法:** : 本文采用的是联合学习方法，通过分布式训练来协作训练查询相似性模型，同时在用户设备上部署本地缓存，以此来实现MeanCache的功能。

- **具体表现:** : 在语义缓存命中与未命中决策任务上，MeanCache的F-score比GPTCache高出约17%，精确度提高了20%，存储需求减少了83%，决策速度提高了11%，这些性能的提升支持了他们的目标。

### 《Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts》

#### [阅读全文](http://arxiv.org/abs/2403.07556v1)

- **研究背景:** : 大型语言模型（Large Language Models, LLMs）在文本生成方面表现出色，但容易受到用户或知识论证工具提供的不真实上下文误导，产生幻觉现象。

- **实现方法:** : 过去的方法没有有效解决LLMs被不真实上下文误导的问题。本文提出的Truth-Aware Context Selection (TACS)方法，通过在输入上下文中进行真实性检测，并基于每个位置的真实性构建相应的注意力掩码，从而选择真实上下文并丢弃不真实上下文。与现有方法相比，TACS是一种轻量级方法，能够更好地动态适应真实和不真实信息，解决了LLMs容易被误导的问题。

- **研究贡献:** : 本文提出了一种新的评估指标——Disturbance Adaption Rate，用于研究LLMs接受真实信息和抵抗不真实信息的能力。同时，实验结果表明TACS能有效过滤上下文中的信息，显著提高LLMs在面对误导信息时的响应质量。

- **研究方法:** : 本文采用的Truth-Aware Context Selection (TACS)方法，首先利用LLMs内部参数化的知识进行输入上下文的真实性检测，然后根据每个位置的真实性构建注意力掩码，选择真实上下文并排除不真实上下文。

- **具体表现:** : 通过在包含误导信息的任务上测试，TACS方法显著提高了LLMs的响应质量。新引入的评估指标Disturbance Adaption Rate进一步证明了TACS在帮助LLMs接受真实信息和抵抗不真实信息方面的有效性。

### 《RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback》

#### [阅读全文](http://arxiv.org/abs/2403.06840v1)

- **研究背景:** : 大型语言模型（LLMs）在许多任务中展现出卓越性能，但仍然严重依赖于存储在其参数中的知识，更新这些知识需要高昂的训练成本。为了解决这一问题，研究者提出了检索增强生成（RAG）方法，通过整合外部知识来提高模型性能。

- **实现方法:** : 过去的方法存在问题，即如果检索到不相关的文本，可能会损害模型性能。本文提出的方法与现有方法的不同之处在于，通过迭代自反馈（RA-ISF）框架，将任务迭代分解并通过三个子模块处理，以增强模型的问题解决能力。该方法通过确保相关知识的有效检索来解决了以往方法的问题，并且有很好的动机。

- **研究贡献:** : 本文提出的RA-ISF方法在现有基准测试中表现优异，尤其是在GPT3.5和Llama2模型上，显著提高了事实推理能力，并减少了幻觉现象。

- **研究方法:** : 本文采用的研究方法是迭代自反馈框架，通过三个子模块迭代处理任务，从而提高模型的问题解决能力。

- **具体表现:** : 本文方法在特定任务上的表现优于现有基准，尤其是在提高事实推理能力和减少幻觉方面。这些性能支持了他们的研究目标。

### 《The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework》

#### [阅读全文](http://arxiv.org/abs/2403.06832v1)

- **研究背景:** : 随着多模态预训练的发展，构建一个稳健的多模态知识图谱（Multi-Modal Knowledge Graph, MMKG）表示学习框架变得尤为重要，以便在大规模多模态大型语言模型（Large Language Models, LLMs）中整合结构化知识，解决知识误解和多模态幻觉等问题。

- **实现方法:** : 以往的方法存在一些问题，如难以准确嵌入MMKG中的实体。本文提出的方法与现有方法的不同之处在于，提出了一种名为SNAG的新方法，该方法采用了基于Transformer的架构，并配备了模态级噪声掩蔽，以稳健地整合多模态实体特征到知识图谱中。该方法通过为多模态知识图谱补全（Multi-modal Knowledge Graph Completion, MKGC）和多模态实体对齐（Multi-modal Entity Alignment, MMEA）定制特定的训练目标，解决了以往方法的问题，并且动机充分。

- **研究贡献:** : 本文提出的SNAG方法在十个数据集上（MKGC的三个和MMEA的七个）取得了最先进（State of the Art, SOTA）的性能，证明了其稳健性和多功能性。此外，SNAG不仅可以作为独立模型使用，还可以增强其他现有方法，提供稳定的性能提升。

- **研究方法:** : 本文采用了基于Transformer的架构，并引入了模态级噪声掩蔽技术，通过特定的训练目标来提高MKGC和MMEA任务的性能。

- **具体表现:** : 在MKGC的三个数据集和MMEA的七个数据集上，SNAG方法均取得了SOTA性能，展现了其在多模态知识图谱表示学习领域的优越性。这些性能结果支持了本文的研究目标。

### 《HILL: A Hallucination Identifier for Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2403.06710v1)

- **研究背景:** : 大型语言模型（LLMs）容易产生幻觉，即无意义、不忠实和不受欢迎的文本。用户往往过度依赖LLMs及其幻觉，这可能导致误解和错误。

- **实现方法:** : 过去的方法没有有效识别和处理LLMs的幻觉问题。本文提出的HILL（Hallucination Identifier for Large Language Models）方法，通过Wizard of Oz方法与用户合作，识别设计特征，并基于这些特征实现HILL，以帮助用户更谨慎地处理LLMs的回应。HILL与现有方法的不同之处在于，它专注于用户中心的设计，以提高LLMs的可靠性。

- **研究贡献:** : 本文提出了HILL，一个可以正确识别和突出LLMs回应中的幻觉的工具，使用户能够更加谨慎地处理这些回应。同时，本文还展示了用户中心AI工具设计的重要性。

- **研究方法:** : 本文首先通过Wizard of Oz方法与9名参与者合作确定HILL的设计特征，然后根据这些特征实现HILL，并通过17名参与者的调查评估HILL的界面设计。此外，还通过现有的问答数据集和5次用户访谈调查了HILL识别幻觉的功能。

- **具体表现:** : HILL能够正确识别和突出LLMs回应中的幻觉，使用户对LLMs的回应更加谨慎。尽管没有提供具体的性能指标，但研究结果支持HILL能够帮助用户更有效地识别和处理LLMs的幻觉，从而达到提高LLMs回应可靠性的目标。

### 《KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation》

#### [阅读全文](http://arxiv.org/abs/2403.06642v1)

- **研究背景:** : 推荐系统领域中利用语义信息是一个重要的研究问题，旨在补充主流基于ID的方法的不足。随着大型语言模型（LLM）的兴起，其作为知识库的能力和推理能力为推荐系统研究带来了新的可能性，使得基于LLM的推荐成为新兴研究方向。

- **实现方法:** : 过去的方法依赖于ID信息，存在问题如幻觉现象，直接使用LLM处理语义信息在推荐场景中不可靠且次优。本文提出的KELLMRec方法与现有方法的不同之处在于，它使用外部知识辅助LLM生成真实可用的文本，并包含一个基于知识的对比学习方案进行训练，以解决上述问题。

- **研究贡献:** : 本文提出了一种知识增强的LLM推荐方法（KELLMRec），通过在提示中使用外部知识，并引入知识为基础的对比学习方案，提高了推荐系统的性能。

- **研究方法:** : 本文采用了知识增强的提示设计和知识为基础的对比学习方案，通过在公共数据集和企业内部数据集上的实验来验证所提方法的有效性。

- **具体表现:** : 通过在公共数据集和企业内部数据集上的实验，KELLMRec方法展示了其有效性。具体的性能指标未在摘要中提及，但实验结果支持了该方法能够提高推荐系统的性能这一目标。

### 《Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2403.06448v1)

- **研究背景:** : 大型语言模型（LLMs）在生成回应时可能会出现连贯但事实不准确的“幻觉”现象，这严重影响了LLMs在实际应用中的有效性。

- **实现方法:** : 过去的方法主要集中在幻觉检测的后处理技术上，这些技术计算量大且由于与LLMs推理过程的分离而效果有限。本文提出的MIND方法不同于现有方法，它是一种无监督的训练框架，利用LLMs的内部状态实现实时幻觉检测，无需人工注释。MIND通过与LLMs的推理过程结合，解决了现有方法的问题，并且具有很好的动机。

- **研究贡献:** : 本文引入了MIND框架，并提出了一个新的基准HELM，用于评估多个LLMs的幻觉检测性能，包括多样化的LLMs输出和LLMs推理过程中的内部状态。

- **研究方法:** : 本文采用了无监督学习框架MIND，通过分析LLMs的内部状态来实时检测幻觉，并使用HELM基准进行评估。

- **具体表现:** : 通过实验表明，MIND在幻觉检测任务上超越了现有的最先进方法。这些性能结果支持了MIND方法实现实时幻觉检测的目标。

### 《Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code》

#### [阅读全文](http://arxiv.org/abs/2403.06675v1)

- **研究背景:** : 本文探讨了基于人工智能(AI)的代码生成器在软件开发中的应用及其安全性问题。由于这些大型语言模型依赖于从不可靠在线源（如GitHub, Hugging Face）收集的大量数据进行训练，它们容易受到数据投毒攻击，即攻击者通过向训练数据中注入恶意样本来破坏模型。

- **实现方法:** : 过去的方法侧重于使用AI模型辅助代码生成，但未充分考虑安全性问题。本文提出了一种新型的数据投毒攻击，能够导致生成含有漏洞的代码。与现有方法相比，本文的方法着重于识别和评估这些攻击对最先进代码生成模型的影响，并探讨了解决这一威胁的潜在解决方案。

- **研究贡献:** : 本文的主要贡献在于识别了AI代码生成器的一个新的安全威胁——数据投毒攻击，并对其进行了广泛的评估，以了解这些攻击如何影响最新的代码生成模型。此外，本文还讨论了可能的解决方案。

- **研究方法:** : 本文采用了定性和定量分析相结合的方法，首先识别潜在的安全威胁，然后通过实验评估这些威胁对AI代码生成器的影响，并探讨了防御措施。

- **具体表现:** : 本文通过实验评估了数据投毒攻击对代码生成模型的影响，但未提供具体的性能指标。尽管如此，研究结果支持了作者关于这种攻击可能对AI代码生成器构成严重威胁的观点，并强调了采取防御措施的必要性。

### 《BOOSTING OF THOUGHTS: TRIAL-AND-ERROR PROBLEM SOLVING WITH LARGE LANGUAGE MODELS》

#### [阅读全文](https://arxiv.org/pdf/2402.11140.pdf)

```
BoT(Boosting of Thoughts)是一种新颖的基于语言模型的自动Prompt设计框架，旨在通过迭代优化的方式逐步增强思维推理链条的质量。与传统的Chain of Thought (CoT)相比，CoT需要依赖人工注释的少量示例推理链条作为prompt输入，而BoT则从一个简单的无示例的prompt出发，通过不断探索、自我评估和修正，逐步优化和积累推理经验，最终生成高质量的思维链条得到正确解。
```

### 《Polaris: A Safety-focused LLM Constellation Architecture for Healthcare》

#### [阅读全文](http://arxiv.org/abs/2403.13313v1)

- **研究背景:** : 本文开发了Polaris，这是首个专注于安全性的实时患者-AI医疗对话的LLM星座架构。以往的医疗LLM研究主要集中在问答任务上，而本研究特别关注长时间的多轮语音对话。

- **实现方法:** : 以往方法在医疗领域的LLM应用存在安全性和幻觉问题。Polaris采用了由数十亿参数的多个LLM合作代理组成的一万亿参数星座系统，包括一个负责推动对话的主要状态代理和多个专注于护理任务的专家支持代理，以提高安全性并减少幻觉。Polaris通过迭代共同训练的复杂协议优化多样化目标，训练数据包括专有数据、临床护理计划、医疗监管文件、医疗手册等医疗推理文档，并模仿医疗专业人员的对话方式。

- **研究贡献:** : Polaris是首个经过全面临床评估的医疗LLM系统。研究团队招募了1100多名美国执业护士和130多名美国执业医师，作为患者对系统进行端到端的对话评估，并在医疗安全性、临床准备、对话质量和床边礼仪等多个维度上进行评分。此外，还对专家支持代理进行了基于任务的挑战性评估。

- **研究方法:** : 采用了迭代共同训练的协议，优化了多样化的目标，并通过与患者演员和经验丰富的护士之间的有机医疗对话和模拟对话来训练模型，使系统能够建立融洽关系、信任、同情心和床边礼仪。

- **具体表现:** : Polaris在医疗安全性、临床准备、对话质量和床边礼仪等维度上的表现与人类护士相当。在对专家支持代理的任务基础评估中，Polaris的LLM代理显著优于更大的通用LLM（GPT-4）以及同等大小的LLaMA-2 70B。这些表现支持了Polaris的研究目标。

### 《Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners》

#### [阅读全文](http://arxiv.org/abs/2403.13198v1)

- **研究背景:** : 本文探讨了大型语言模型（LLMs）在智能机器人中的应用，特别是它们在预测时可能产生的幻觉问题，以及如何使机器人在需要帮助时能够察觉并请求帮助。

- **实现方法:** : 过去的方法存在机器人执行与用户目标相悖的计划、过度依赖人类协助或无法适时寻求帮助的问题。本文提出了一种新的方法LAP（基于场景和物体可供性的规划器），通过计算场景可供性分数来减少LLMs的幻觉预测，使LLMs的信心度与成功概率更加一致。LAP与现有方法的不同之处在于，它结合了LLMs和场景可供性分数来提高预测的准确性，并知道何时请求帮助。

- **研究贡献:** : 本文提出了三种不同的可供性分数计算方法，可以单独或联合使用以提高不同用例的性能。实验表明，LAP显著提高了成功率，并减少了人类干预的需求。

- **研究方法:** : 本文通过模拟实验和真实世界中的任务测试，验证了LAP方法的有效性。这些任务包含了各种不确定性，以测试LAP在实际应用中的表现。

- **具体表现:** : 在真实世界的测试中，LAP相比之前的方法，在70%的成功率下将人类帮助率降低了超过33%。这表明LAP能够有效减少机器人对人类的依赖，同时保持较高的任务成功率，支持了研究目标的实现

### [代理幻觉优化]《Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models》

#### [阅读全文](http://arxiv.org/abs/2403.12881v1)

- **研究背景:** : 开源的大型语言模型（LLMs）在多种自然语言处理（NLP）任务中取得了巨大成功，但作为代理（agent）时仍然远不如基于API的模型。如何将代理能力整合到通用的LLMs中，成为一个紧迫且关键的问题。

- **实现方法:** : 过去的方法存在问题，比如训练语料库中格式遵循和代理推理混杂，导致与预训练数据分布显著不同；LLMs在代理任务所需能力上学习速度不一；当前方法在提高代理能力时会引入幻觉（hallucinations）。Agent-FLAN通过仔细分解和重新设计训练语料库，解决了这些问题，并且在提高代理能力的同时，减少了幻觉问题。

- **研究贡献:** : Agent-FLAN使得Llama2-7B模型在各种代理评估数据集上的表现超过了之前最好的工作，提升了3.5%。通过综合构建的负样本，大幅缓解了幻觉问题，并建立了评估基准。此外，Agent-FLAN在扩大模型规模时一致提高了LLMs的代理能力，同时略微增强了LLMs的通用能力。

- **研究方法:** : Agent-FLAN通过对训练语料库的细致分解和重新设计，使得模型更有效地学习代理任务所需的能力。同时，通过构建负样本来减少幻觉问题，并建立了一个评估基准来衡量改进效果。

- **具体表现:** : Agent-FLAN应用于Llama2-7B模型，在多个代理评估数据集上的表现提升了3.5%，这一性能提升支持了他们的目标，即有效地提升LLMs作为代理的能力，并减少幻觉问题。

### [自我纠正]《QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction》

#### [阅读全文](http://arxiv.org/abs/2403.11886v1)

- **研究背景:** : 本文针对Large Language Models (LLMs) 在语义解析中应用时存在的可靠性和效率问题，特别是在遇到幻觉现象时的不足。

- **实现方法:** : 过去的方法在处理幻觉现象时缺乏可靠性和效率。本文提出的QueryAgent框架通过逐步解决问题并进行逐步自我纠正来解决这些问题。与传统方法不同，QueryAgent引入了一种基于环境反馈的自我纠正方法ERASER，它利用中间步骤中的丰富环境反馈，仅在必要时进行选择性和差异化的自我纠正。该方法的动机是明确的，旨在提高LLMs的可靠性和效率。

- **研究贡献:** : QueryAgent在GrailQA和GraphQ数据集上使用仅一个示例的情况下，相比以往的少数样本方法，分别提高了7.0和15.0 F1的性能。此外，通过使用ERASER，还将另一个基线AgentBench的性能提高了约10个百分点，显示了该方法的强大迁移能力。

- **研究方法:** : 本文提出了一种新的框架QueryAgent，它通过逐步解决问题和逐步自我纠正来进行语义解析。引入了ERASER方法，该方法利用中间步骤的环境反馈进行自我纠正。

- **具体表现:** : 在GrailQA和GraphQ任务上，QueryAgent使用一个示例的少数样本方法分别取得了7.0和15.0 F1的提升。ERASER进一步提升了AgentBench约10个百分点的性能。这些性能结果支持了本文提出方法的目标。
